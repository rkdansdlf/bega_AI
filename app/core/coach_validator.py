"""
Coach ì‘ë‹µ ê²€ì¦ ëª¨ë“ˆ.

COACH_PROMPT_V2ì˜ JSON ì¶œë ¥ì„ Pydantic ëª¨ë¸ë¡œ ê²€ì¦í•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤.
LLM ì¶œë ¥ì˜ ì¼ê´€ì„±ì„ ë³´ì¥í•˜ê³ , ì˜ëª»ëœ í˜•ì‹ì˜ ì‘ë‹µì„ ê°ì§€í•©ë‹ˆë‹¤.
"""

import json
import logging
import re
from copy import deepcopy
from typing import Any, Dict, List, Literal, Optional, Union, Tuple
from pydantic import BaseModel, Field, field_validator, BeforeValidator
from typing_extensions import Annotated

logger = logging.getLogger(__name__)
MAX_KEY_METRIC_VALUE_LENGTH = 50
MAX_CRITICAL_METRICS = 2
FALLBACK_HEADLINE = "AI ì½”ì¹˜ ë¶„ì„ ìš”ì•½"


# ============================================================
# Pydantic Models for Coach Response
# ============================================================


class KeyMetric(BaseModel):
    """í•µì‹¬ ì§€í‘œ ëª¨ë¸"""

    label: str = Field(..., max_length=30, description="ì§€í‘œëª…")
    # ë¬¸ìì—´ë¡œ ìë™ ë³€í™˜ (LLMì´ ìˆ«ìë¡œ ì£¼ëŠ” ê²½ìš° ëŒ€ë¹„)
    value: Annotated[Union[str, int, float], BeforeValidator(str)] = Field(
        ..., max_length=MAX_KEY_METRIC_VALUE_LENGTH, description="ìˆ˜ì¹˜"
    )
    status: Literal["good", "warning", "danger"] = Field(
        default="warning", description="í‰ê°€ (good/warning/danger)"
    )
    trend: Literal["up", "down", "neutral"] = Field(default="neutral")
    is_critical: bool = Field(default=False)

    @field_validator("status", mode="before")
    @classmethod
    def normalize_status(cls, v: str) -> str:
        """ìƒíƒœ ê°’ì„ ì˜ì–´ë¡œ ì •ê·œí™” (í•œê¸€â†’ì˜ì–´ í†µì¼)"""
        if not isinstance(v, str):
            return "warning"
        normalized = v.lower().strip()
        if normalized in ["ì–‘í˜¸", "good", "positive", "ìµœìƒ"]:
            return "good"
        elif normalized in ["ì£¼ì˜", "warning", "caution", "ë³´í†µ"]:
            return "warning"
        elif normalized in ["ìœ„í—˜", "danger", "critical", "bad"]:
            return "danger"
        return "warning"  # ì•Œ ìˆ˜ ì—†ëŠ” ê°’ì€ warningìœ¼ë¡œ ê¸°ë³¸ ì²˜ë¦¬

    @field_validator("trend", mode="before")
    @classmethod
    def normalize_trend(cls, v: str) -> str:
        """ì¶”ì„¸ ê°’ì„ í—ˆìš© ìŠ¤í‚¤ë§ˆë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤."""
        if not isinstance(v, str):
            return "neutral"
        normalized = v.lower().strip()
        if normalized in ["up", "ìƒìŠ¹", "increase", "rising"]:
            return "up"
        if normalized in ["down", "í•˜ë½", "decrease", "falling"]:
            return "down"
        if normalized in ["neutral", "ë³´í•©", "ìœ ì§€", "stable", "flat"]:
            return "neutral"
        return "neutral"


class RiskItem(BaseModel):
    """ìœ„í—˜ ìš”ì†Œ ëª¨ë¸"""

    area: str = Field(..., description="ì˜ì—­ (bullpen/starter/batting/defense)")
    level: Literal[0, 1, 2] = Field(..., description="ìœ„í—˜ë„ (0=ìœ„í—˜, 1=ì£¼ì˜, 2=ì–‘í˜¸)")
    description: str = Field(..., max_length=150, description="ìœ„í—˜ ì„¤ëª… (ìµœëŒ€ 150ì)")

    @field_validator("area", mode="before")
    @classmethod
    def normalize_area(cls, v: str) -> str:
        """ì˜ì—­ ê°’ì„ ì˜ì–´ë¡œ ì •ê·œí™”"""
        if not isinstance(v, str):
            return "overall"
        normalized = v.lower().strip()
        area_mapping = {
            "ë¶ˆíœ": "bullpen",
            "bullpen": "bullpen",
            "ë¦´ë¦¬í”„": "bullpen",
            "ì„ ë°œ": "starter",
            "starter": "starter",
            "starting": "starter",
            "íƒ€ê²©": "batting",
            "batting": "batting",
            "íƒ€ì„ ": "batting",
            "ìˆ˜ë¹„": "defense",
            "defense": "defense",
            "ì „ì²´": "overall",
            "overall": "overall",
        }
        return area_mapping.get(normalized, "overall")


class AnalysisSection(BaseModel):
    """ë¶„ì„ ì„¹ì…˜ ëª¨ë¸"""

    strengths: List[str] = Field(default_factory=list, description="ê°•ì  ëª©ë¡")
    weaknesses: List[str] = Field(default_factory=list, description="ì•½ì  ëª©ë¡")
    risks: List[RiskItem] = Field(default_factory=list, description="ìœ„í—˜ ìš”ì†Œ ëª©ë¡")


class CoachResponse(BaseModel):
    """
    Coach ì‘ë‹µ ì „ì²´ ëª¨ë¸.

    COACH_PROMPT_V2ì˜ JSON ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
    """

    headline: str = Field(
        ..., min_length=5, max_length=60, description="í•œ ì¤„ ì§„ë‹¨ (ìµœëŒ€ 60ì)"
    )
    sentiment: Literal["positive", "negative", "neutral"] = Field(default="neutral")
    key_metrics: List[KeyMetric] = Field(
        default_factory=list, description="í•µì‹¬ ì§€í‘œ ëª©ë¡ (ìµœëŒ€ 6ê°œ)"
    )
    analysis: AnalysisSection = Field(default_factory=AnalysisSection)
    detailed_markdown: str = Field(
        default="", max_length=500, description="ìƒì„¸ ë¶„ì„ ë§ˆí¬ë‹¤ìš´ (ìµœëŒ€ 500ì)"
    )
    coach_note: str = Field(
        default="", max_length=120, description="ì „ëµì  ì œì–¸ (ìµœëŒ€ 120ì)"
    )

    @field_validator("headline", mode="before")
    @classmethod
    def truncate_headline(cls, v: str) -> str:
        """headline ê¸¸ì´ ì œí•œ ë° ì •ë¦¬"""
        if not isinstance(v, str) or not v.strip():
            raise ValueError("headlineì€ ë¹„ì–´ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        v = v.strip()

        # [Fix] "headline": "Title" í˜•íƒœì˜ ì¤‘ë³µ í‚¤ íŒ¨í„´ ì œê±°
        # LLMì´ JSON í˜•ì‹ì„ ê°’ ì•ˆì— í¬í•¨ì‹œí‚¤ëŠ” í™˜ê° ë°©ì§€
        import re

        dup_pattern = r'^"headline"\s*:\s*"(.*)"$'
        match = re.match(dup_pattern, v)
        if match:
            v = match.group(1)

        # [Fix] ë”°ì˜´í‘œë¡œ ê°ì‹¸ì§„ ê²½ìš° ì œê±°
        if v.startswith('"') and v.endswith('"'):
            v = v[1:-1]

        if len(v) > 60:
            v = v[:57] + "..."
        return v

    @field_validator("detailed_markdown", mode="before")
    @classmethod
    def truncate_markdown(cls, v: str) -> str:
        """detailed_markdown ê¸¸ì´ ì œí•œ (í”„ë¡¬í”„íŠ¸ ê·œì¹™: ìµœëŒ€ 500ì)"""
        if not isinstance(v, str):
            return ""
        v = v.strip()
        if len(v) > 500:
            v = v[:497] + "..."
        return v

    @field_validator("coach_note", mode="before")
    @classmethod
    def truncate_coach_note(cls, v: str) -> str:
        """coach_note ê¸¸ì´ ì œí•œ (í”„ë¡¬í”„íŠ¸ ê·œì¹™: ìµœëŒ€ 120ì)"""
        if not isinstance(v, str):
            return ""
        v = v.strip()
        if len(v) > 120:
            v = v[:117] + "..."
        return v

    @field_validator("key_metrics", mode="before")
    @classmethod
    def limit_metrics(cls, v: list) -> list:
        """key_metrics ê°œìˆ˜ ì œí•œ (ìµœëŒ€ 6ê°œ)"""
        if not isinstance(v, list):
            return []
        return v[:6]


# ============================================================
# Parser Functions
# ============================================================


def extract_json_from_response(raw_response: str) -> Optional[str]:
    """
    LLM ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    ë‹¤ì–‘í•œ í˜•ì‹ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤:
    - ìˆœìˆ˜ JSON
    - ```json ... ``` ì½”ë“œ ë¸”ë¡
    - ì•ë’¤ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” JSON
    """
    if not raw_response:
        return None

    text = raw_response.strip()

    # Case 1: ```json ì½”ë“œ ë¸”ë¡
    json_block_pattern = r"```(?:json)?\s*([\s\S]*?)```"
    matches = re.findall(json_block_pattern, text)
    if matches:
        return matches[0].strip()

    # Case 2: { ... } JSON ê°ì²´ ì§ì ‘ ì°¾ê¸°
    # ê°€ì¥ ë°”ê¹¥ìª½ ì¤‘ê´„í˜¸ ë§¤ì¹­
    brace_count = 0
    start_idx = -1
    end_idx = -1

    for i, char in enumerate(text):
        if char == "{":
            if brace_count == 0:
                start_idx = i
            brace_count += 1
        elif char == "}":
            brace_count -= 1
            if brace_count == 0 and start_idx != -1:
                end_idx = i + 1
                break

    if start_idx != -1 and end_idx != -1:
        return text[start_idx:end_idx]

    return None


def derive_headline(data: Dict[str, Any]) -> str:
    """í—¤ë“œë¼ì¸ ëˆ„ë½ ì‹œ ëŒ€ì²´ í—¤ë“œë¼ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    raw_headline = data.get("headline")
    if isinstance(raw_headline, str) and raw_headline.strip():
        return raw_headline.strip()

    key_metrics = data.get("key_metrics")
    if isinstance(key_metrics, list) and key_metrics:
        first_metric = key_metrics[0]
        if isinstance(first_metric, dict):
            label = str(first_metric.get("label", "")).strip()
            value = str(first_metric.get("value", "")).strip()
            if label and value:
                return f"{label} {value} ì¤‘ì‹¬ ë¶„ì„"
            if label:
                return f"{label} ì¤‘ì‹¬ ë¶„ì„"

    analysis = data.get("analysis")
    if isinstance(analysis, dict):
        for key in ("strengths", "weaknesses"):
            items = analysis.get(key)
            if isinstance(items, list):
                for item in items:
                    if isinstance(item, str) and item.strip():
                        return item.strip()

    return FALLBACK_HEADLINE


def normalize_key_metric_values(data: Dict[str, Any]) -> List[str]:
    """key_metrics[].valueë¥¼ ë¬¸ìì—´í™”í•˜ê³  ê¸¸ì´ë¥¼ ì œí•œí•©ë‹ˆë‹¤."""
    reasons: List[str] = []
    metrics = data.get("key_metrics")
    if not isinstance(metrics, list):
        return reasons

    for idx, metric in enumerate(metrics):
        if not isinstance(metric, dict):
            continue
        if "value" not in metric:
            continue
        value = str(metric.get("value", "")).strip()
        if len(value) > MAX_KEY_METRIC_VALUE_LENGTH:
            metric["value"] = value[: MAX_KEY_METRIC_VALUE_LENGTH - 3] + "..."
            reasons.append(f"truncate_key_metrics_value_{idx}")
        else:
            metric["value"] = value
    return reasons


def normalize_critical_flags(
    data: Dict[str, Any], max_critical: int = MAX_CRITICAL_METRICS
) -> List[str]:
    """is_critical=trueë¥¼ ìµœëŒ€ ê°œìˆ˜ë§Œ ìœ ì§€í•©ë‹ˆë‹¤."""
    reasons: List[str] = []
    metrics = data.get("key_metrics")
    if not isinstance(metrics, list):
        return reasons

    critical_indices: List[int] = []
    for idx, metric in enumerate(metrics):
        if isinstance(metric, dict) and bool(metric.get("is_critical")):
            critical_indices.append(idx)

    if len(critical_indices) > max_critical:
        for idx in critical_indices[max_critical:]:
            metric = metrics[idx]
            if isinstance(metric, dict):
                metric["is_critical"] = False
        reasons.append(f"reduce_is_critical_{len(critical_indices)}_to_{max_critical}")
    return reasons


def normalize_coach_payload(data: dict) -> Tuple[dict, List[str]]:
    """LLM ì‘ë‹µ payloadë¥¼ ê²€ì¦ ì „ì— ì •ê·œí™”í•©ë‹ˆë‹¤."""
    if not isinstance(data, dict):
        return data, []

    normalized = deepcopy(data)
    reasons: List[str] = []

    if not isinstance(normalized.get("key_metrics"), list):
        normalized["key_metrics"] = []
        reasons.append("coerce_key_metrics_to_empty_list")

    headline = normalized.get("headline")
    if not isinstance(headline, str) or not headline.strip():
        normalized["headline"] = derive_headline(normalized)
        reasons.append("derive_headline")

    reasons.extend(normalize_key_metric_values(normalized))
    reasons.extend(normalize_critical_flags(normalized))
    return normalized, reasons


def classify_parse_error(error_message: Optional[str]) -> str:
    """íŒŒì‹± ì‹¤íŒ¨ ë©”ì‹œì§€ë¥¼ í‘œì¤€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    if not error_message:
        return "unknown"

    lowered = error_message.lower()
    if "empty response" in lowered:
        return "empty_response"
    if "no json found" in lowered:
        return "no_json_found"
    if "json decode error" in lowered:
        return "json_decode_error"
    if "headline" in lowered and "field required" in lowered:
        return "schema_missing_headline"
    if (
        "key_metrics" in lowered
        and "value" in lowered
        and ("at most 50" in lowered or "too_long" in lowered)
    ):
        return "metric_value_too_long"
    if "validation error" in lowered:
        return "schema_validation_error"
    return "unknown"


def parse_coach_response_with_meta(
    raw_response: str,
) -> Tuple[Optional[CoachResponse], Optional[str], Dict[str, Any]]:
    """
    parse_coach_responseì˜ í™•ì¥ ë²„ì „ì…ë‹ˆë‹¤.
    ì •ê·œí™” ì ìš© ì—¬ë¶€ì™€ ì‹¤íŒ¨ ì¹´í…Œê³ ë¦¬ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    meta: Dict[str, Any] = {
        "normalization_applied": False,
        "normalization_reasons": [],
        "error_code": None,
    }

    if not raw_response or not raw_response.strip():
        error = "Empty response"
        meta["error_code"] = classify_parse_error(error)
        return None, error, meta

    try:
        json_str = extract_json_from_response(raw_response)

        if not json_str and raw_response.strip().startswith('"headline"'):
            logger.warning(
                "[CoachValidator] Missing braces detected, attempting to wrap with {}"
            )
            temp_json = "{" + raw_response.strip()
            if not temp_json.endswith("}"):
                temp_json += "}"
            try:
                data = json.loads(temp_json)
                if not isinstance(data, dict):
                    error = "Validation error: root JSON must be an object"
                    meta["error_code"] = classify_parse_error(error)
                    return None, error, meta
                normalized_data, reasons = normalize_coach_payload(data)
                meta["normalization_reasons"] = reasons
                meta["normalization_applied"] = len(reasons) > 0
                return CoachResponse(**normalized_data), None, meta
            except Exception as e:
                logger.warning(f"Fallback parsing failed: {e}")

        if not json_str:
            error = "No JSON found"
            meta["error_code"] = classify_parse_error(error)
            return None, error, meta

        try:
            data = json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.warning(f"[CoachValidator] JSON decode error: {e}")
            error = f"JSON decode error: {e}"
            meta["error_code"] = classify_parse_error(error)
            return None, error, meta

        if not isinstance(data, dict):
            error = "Validation error: root JSON must be an object"
            meta["error_code"] = classify_parse_error(error)
            return None, error, meta

        normalized_data, reasons = normalize_coach_payload(data)
        meta["normalization_reasons"] = reasons
        meta["normalization_applied"] = len(reasons) > 0

        try:
            return CoachResponse(**normalized_data), None, meta
        except Exception as e:
            logger.warning(f"[CoachValidator] Pydantic validation error: {e}")
            error = f"Validation error: {e}"
            meta["error_code"] = classify_parse_error(error)
            return None, error, meta

    except Exception as e:
        logger.error(f"[CoachValidator] Failed to parse coach response: {e}")
        error = f"Unknown error: {e}"
        meta["error_code"] = classify_parse_error(error)
        return None, error, meta


def parse_coach_response(
    raw_response: str,
) -> Tuple[Optional[CoachResponse], Optional[str]]:
    """
    LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ CoachResponse ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        raw_response: LLMì˜ ì›ì‹œ ì‘ë‹µ ë¬¸ìì—´

    Returns:
        CoachResponse ê°ì²´. íŒŒì‹± ì‹¤íŒ¨ ì‹œ None ë°˜í™˜ (ìºì‹œ FAILED ì²˜ë¦¬ìš©).
    """
    response, error, _ = parse_coach_response_with_meta(raw_response)
    return response, error


def _create_fallback_response(error_reason: str, original_text: str) -> CoachResponse:
    """
    íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë³´ì¡´í•˜ëŠ” fallback ì‘ë‹µ ìƒì„±.

    ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ì‚¬ìš©ìì—ê²Œ ìµœì†Œí•œì˜ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ ìˆëŠ” ì²« ì¤„ ì¶”ì¶œ ì‹œë„
    first_meaningful_line = ""
    if original_text:
        for line in original_text.strip().split("\n"):
            cleaned = line.strip()
            # [Fix] '{', '```', '#' ë¿ë§Œ ì•„ë‹ˆë¼ '"headline"' ê°™ì€ JSON fragmentë„ ë¬´ì‹œ
            if cleaned and not cleaned.startswith(("{", "```", "#", '"')):
                first_meaningful_line = cleaned[:100]  # ìµœëŒ€ 100ì
                break

    headline = first_meaningful_line or "AI ë¶„ì„ ê²°ê³¼"

    # ì›ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡, JSON ì”í•´ ì œê±°)
    cleaned_text = original_text.strip() if original_text else ""
    for prefix in ["```json", "```", "{"]:
        if cleaned_text.startswith(prefix):
            cleaned_text = cleaned_text[len(prefix) :]
    for suffix in ["```", "}"]:
        if cleaned_text.endswith(suffix):
            cleaned_text = cleaned_text[: -len(suffix)]
    cleaned_text = cleaned_text.strip()

    return CoachResponse(
        headline=headline,
        sentiment="neutral",
        key_metrics=[],
        analysis=AnalysisSection(strengths=[], weaknesses=[], risks=[]),
        detailed_markdown="",
        coach_note=(
            cleaned_text[:2000] if cleaned_text else f"í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨: {error_reason}"
        ),
    )


def validate_coach_response(response: CoachResponse) -> List[str]:
    """
    CoachResponseì˜ ë°ì´í„° í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤.

    Returns:
        ê²½ê³  ë©”ì‹œì§€ ëª©ë¡ (ë¹„ì–´ìˆìœ¼ë©´ ì–‘í˜¸)
    """
    warnings = []

    # í•µì‹¬ ì§€í‘œ ê°œìˆ˜ í™•ì¸
    critical_count = sum(1 for m in response.key_metrics if m.is_critical)
    if critical_count > 2:
        warnings.append(
            f"í•µì‹¬ ì§€í‘œ(is_critical=true)ê°€ {critical_count}ê°œì…ë‹ˆë‹¤. ìµœëŒ€ 2ê°œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
        )

    # ë¶„ì„ ë‚´ìš© í™•ì¸
    if not response.analysis.strengths and not response.analysis.weaknesses:
        warnings.append("ê°•ì ê³¼ ì•½ì ì´ ëª¨ë‘ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    # coach_note ê¸¸ì´ í™•ì¸
    if len(response.coach_note) < 20:
        warnings.append("coach_noteê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì „ëµ ì œì–¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")

    # ì„ ìˆ˜ëª… í¬í•¨ ì—¬ë¶€ í™•ì¸ (í’ˆì§ˆ ì§€í‘œ)
    all_text = " ".join(response.analysis.strengths + response.analysis.weaknesses)
    if all_text:
        # í•œê¸€ ì´ë¦„ íŒ¨í„´ (2-4ê¸€ì í•œê¸€ ì´ë¦„)
        korean_name_pattern = r"[ê°€-í£]{2,4}"
        if not re.search(korean_name_pattern, all_text):
            warnings.append("ë¶„ì„ì— ì„ ìˆ˜ëª…ì´ í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. êµ¬ì²´ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")

        # ìˆ˜ì¹˜ ë°ì´í„° í¬í•¨ ì—¬ë¶€ í™•ì¸
        number_pattern = r"\d+\.?\d*"
        if not re.search(number_pattern, all_text):
            warnings.append("ë¶„ì„ì— ìˆ˜ì¹˜ ë°ì´í„°ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return warnings


def format_coach_response_as_markdown(response: CoachResponse) -> str:
    """
    CoachResponseë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    JSON ì‘ë‹µì„ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë Œë”ë§í•˜ê¸° ì¢‹ì€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    parts = []

    # í—¤ë“œë¼ì¸
    sentiment_emoji = {"positive": "ğŸŸ¢", "negative": "ğŸ”´", "neutral": "ğŸŸ¡"}.get(
        response.sentiment, "âšª"
    )
    parts.append(f"## {sentiment_emoji} {response.headline}\n")

    # í•µì‹¬ ì§€í‘œ í…Œì´ë¸”
    if response.key_metrics:
        parts.append("### í•µì‹¬ ì§€í‘œ")
        parts.append("| ì§€í‘œ | ìˆ˜ì¹˜ | ìƒíƒœ | ì¶”ì„¸ |")
        parts.append("|------|------|------|------|")

        trend_symbol = {"up": "ğŸ“ˆ", "down": "ğŸ“‰", "neutral": "â¡ï¸"}
        for m in response.key_metrics:
            critical_mark = "**" if m.is_critical else ""
            trend = trend_symbol.get(m.trend, "")
            parts.append(
                f"| {critical_mark}{m.label}{critical_mark} | {m.value} | {m.status} | {trend} |"
            )
        parts.append("")

    # ë¶„ì„ ì„¹ì…˜
    if response.analysis.strengths:
        parts.append("### ğŸ’ª ê°•ì ")
        for s in response.analysis.strengths:
            parts.append(f"- {s}")
        parts.append("")

    if response.analysis.weaknesses:
        parts.append("### âš ï¸ ì•½ì ")
        for w in response.analysis.weaknesses:
            parts.append(f"- {w}")
        parts.append("")

    if response.analysis.risks:
        parts.append("### ğŸš¨ ìœ„í—˜ ìš”ì†Œ")
        risk_emoji = {0: "ğŸ”´", 1: "ğŸŸ¡", 2: "ğŸŸ¢"}
        for r in response.analysis.risks:
            emoji = risk_emoji.get(r.level, "âšª")
            parts.append(f"- {emoji} **{r.area}**: {r.description}")
        parts.append("")

    # ìƒì„¸ ë¶„ì„ (ì´ë¯¸ ë§ˆí¬ë‹¤ìš´)
    if response.detailed_markdown:
        parts.append(response.detailed_markdown)
        parts.append("")

    # Coach's Note
    if response.coach_note:
        parts.append("### ğŸ’¡ Coach's Note")
        parts.append(response.coach_note)
        parts.append("")

    return "\n".join(parts)
