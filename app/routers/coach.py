"""
'The Coach' ê¸°ëŠ¥ê³¼ ê´€ë ¨ëœ API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

Fast Path ìµœì í™”:
- ë„êµ¬ ê³„íš LLM í˜¸ì¶œì„ ê±´ë„ˆë›°ê³  focus ì˜ì—­ì— ë”°ë¼ ì§ì ‘ ë„êµ¬ í˜¸ì¶œ
- ë³‘ë ¬ ë„êµ¬ ì‹¤í–‰ìœ¼ë¡œ ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶•
- Coach ì „ìš© ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
"""

import logging
import json
import asyncio
import uuid
from time import perf_counter
from datetime import datetime
from typing import List, Optional, Dict, Any, Literal
from fastapi import APIRouter, Body, Depends, HTTPException
from pydantic import BaseModel, model_validator

from psycopg_pool import ConnectionPool

from ..deps import (
    get_agent,
    get_db_connection,
    get_connection_pool,
    get_coach_llm_generator,
)
from ..config import get_settings
from ..agents.baseball_agent import BaseballStatisticsAgent
from ..core.prompts import COACH_PROMPT_V2
from ..core.coach_validator import (
    parse_coach_response,
    CoachResponse,
    _create_fallback_response,
)
from ..core.coach_cache_key import build_coach_cache_key, normalize_focus
from ..core.ratelimit import rate_limit_dependency
from ..tools.database_query import DatabaseQueryTool
from ..tools.team_code_resolver import CANONICAL_CODES, TeamCodeResolver

logger = logging.getLogger(__name__)

# ë¹ˆ ì‘ë‹µ ì‹œ ì¬ì‹œë„ íšŸìˆ˜
MAX_RETRY_ON_EMPTY = 2
COACH_CACHE_SCHEMA_VERSION = "v3"
COACH_CACHE_PROMPT_VERSION = "v5_focus"
COACH_YEAR_MIN = 1982
PENDING_STALE_SECONDS = 300
PENDING_WAIT_TIMEOUT_SECONDS = 10
PENDING_WAIT_POLL_MS = 300
FAILED_RETRY_AFTER_SECONDS = 3600  # FAILED í•­ëª© 1ì‹œê°„ í›„ ìë™ ì¬ì‹œë„ í—ˆìš©
FOCUS_SECTION_HEADERS: Dict[str, str] = {
    "recent_form": "## ìµœê·¼ ì „ë ¥",
    "bullpen": "## ë¶ˆíœ ìƒíƒœ",
    "starter": "## ì„ ë°œ íˆ¬ìˆ˜",
    "matchup": "## ìƒëŒ€ ì „ì ",
    "batting": "## íƒ€ê²© ìƒì‚°ì„±",
}


def _cache_status_response(
    *,
    headline: str,
    coach_note: str,
    detail: str,
) -> Dict[str, Any]:
    return {
        "headline": headline,
        "sentiment": "neutral",
        "key_metrics": [],
        "analysis": {
            "strengths": [],
            "weaknesses": [],
            "risks": [],
        },
        "detailed_markdown": detail,
        "coach_note": coach_note,
    }


def _calc_row_age_seconds(updated_at: Any) -> float:
    if not isinstance(updated_at, datetime):
        return float("inf")
    now_ref = (
        datetime.now(updated_at.tzinfo)
        if updated_at.tzinfo is not None
        else datetime.now()
    )
    return max(0.0, (now_ref - updated_at).total_seconds())


def _determine_cache_gate(
    *,
    status: Optional[str],
    has_cached_json: bool,
    updated_at: Any,
    pending_stale_seconds: int = PENDING_STALE_SECONDS,
) -> str:
    if status == "COMPLETED" and has_cached_json:
        return "HIT"
    if status == "PENDING":
        age_seconds = _calc_row_age_seconds(updated_at)
        if age_seconds > pending_stale_seconds:
            return "PENDING_STALE_TAKEOVER"
        return "PENDING_WAIT"
    if status == "FAILED":
        age_seconds = _calc_row_age_seconds(updated_at)
        if age_seconds > FAILED_RETRY_AFTER_SECONDS:
            return "MISS_GENERATE"  # 1ì‹œê°„ ê²½ê³¼ ì‹œ ì¬ìƒì„± í—ˆìš©
        return "FAILED_LOCKED"
    return "MISS_GENERATE"


def _should_generate_from_gate(gate: str) -> bool:
    return gate in {"MISS_GENERATE", "PENDING_STALE_TAKEOVER"}


async def _wait_for_cache_terminal_state(
    pool: ConnectionPool,
    cache_key: str,
    timeout_seconds: float = PENDING_WAIT_TIMEOUT_SECONDS,
    poll_ms: int = PENDING_WAIT_POLL_MS,
) -> Optional[Dict[str, Any]]:
    deadline = perf_counter() + timeout_seconds
    sleep_seconds = max(float(poll_ms), 1.0) / 1000.0

    while perf_counter() < deadline:
        await asyncio.sleep(sleep_seconds)
        try:
            with pool.connection() as conn:
                row = conn.execute(
                    """
                    SELECT status, response_json, error_message
                    FROM coach_analysis_cache
                    WHERE cache_key = %s
                    """,
                    (cache_key,),
                ).fetchone()
            if not row:
                continue
            status, cached_json, error_message = row
            if status == "COMPLETED" and cached_json:
                return {
                    "status": "COMPLETED",
                    "response_json": cached_json,
                }
            if status == "FAILED":
                return {
                    "status": "FAILED",
                    "error_message": error_message,
                }
        except Exception as exc:
            logger.warning("[Coach] Cache wait poll failed for %s: %s", cache_key, exc)
            return None
    return None


def _normalize_cached_response(cached_data: dict) -> dict:
    """
    ë ˆê±°ì‹œ ìºì‹œ ë°ì´í„°ë¥¼ í˜„ì¬ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì •ê·œí™”í•©ë‹ˆë‹¤.

    CoachResponse ê²€ì¦ê¸°ë¥¼ í†µê³¼ì‹œì¼œ ìë™ìœ¼ë¡œ ë³€í™˜:
    - status: "ì£¼ì˜" â†’ "warning", "ì–‘í˜¸" â†’ "good", "ìœ„í—˜" â†’ "danger"
    - area: "ë¶ˆíœ" â†’ "bullpen", "ì„ ë°œ" â†’ "starter", "íƒ€ê²©" â†’ "batting"
    - coach_note: 150ì ì´ˆê³¼ ì‹œ ìë™ truncate

    Args:
        cached_data: ìºì‹œì—ì„œ ì½ì€ ì›ë³¸ JSON ë°ì´í„°

    Returns:
        ì •ê·œí™”ëœ ë°ì´í„° (ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜)
    """
    if not cached_data:
        return cached_data

    try:
        # CoachResponse ê²€ì¦ê¸°ë¥¼ í†µê³¼ì‹œì¼œ ìë™ ì •ê·œí™”
        response = CoachResponse(**cached_data)
        normalized = response.model_dump()
        logger.debug("[Coach Cache] Normalized legacy data")
        return normalized
    except Exception as e:
        logger.warning(f"[Coach Cache] Failed to normalize legacy data: {e}")
        return cached_data


def _parse_explicit_year(value: Any) -> Optional[int]:
    """ëª…ì‹œì  year í•„ë“œ(ì˜ˆ: season_year)ë¥¼ ì •ìˆ˜ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤."""
    if value is None or isinstance(value, bool):
        return None
    try:
        year = int(str(value).strip())
    except (TypeError, ValueError):
        return None
    if 1000 <= year <= 9999:
        return year
    return None


def _parse_year_from_date_like(value: Any) -> Optional[int]:
    """YYYY-MM-DD ë˜ëŠ” YYYY... í˜•íƒœ ë¬¸ìì—´ì—ì„œ ì—°ë„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if value is None:
        return None
    text = str(value).strip()
    if len(text) < 4 or not text[:4].isdigit():
        return None
    return int(text[:4])


def _is_valid_analysis_year(year: int) -> bool:
    return COACH_YEAR_MIN <= year <= datetime.now().year + 1


def _resolve_year_from_season_id(pool: ConnectionPool, season_id: Any) -> Optional[int]:
    if season_id is None:
        return None
    try:
        normalized_season_id = int(str(season_id).strip())
    except (TypeError, ValueError):
        return None

    try:
        with pool.connection() as conn:
            row = conn.execute(
                "SELECT season_year FROM kbo_seasons WHERE season_id = %s LIMIT 1",
                (normalized_season_id,),
            ).fetchone()
        if not row:
            return None
        season_year = int(row[0])
        return season_year
    except Exception as exc:
        logger.warning(
            "[Coach Router] Failed to resolve season_id=%s: %s", season_id, exc
        )
        return None


def _resolve_year_from_game_context(
    pool: ConnectionPool, game_id: Optional[str], game_date: Any
) -> Optional[int]:
    explicit_game_year = _parse_year_from_date_like(game_date)
    if explicit_game_year is not None:
        return explicit_game_year

    if game_id:
        try:
            with pool.connection() as conn:
                row = conn.execute(
                    "SELECT game_date FROM game WHERE game_id = %s LIMIT 1",
                    (game_id,),
                ).fetchone()
            if row and row[0]:
                game_date_obj = row[0]
                if hasattr(game_date_obj, "year"):
                    return int(game_date_obj.year)
                return _parse_year_from_date_like(game_date_obj)
        except Exception as exc:
            logger.warning(
                "[Coach Router] Failed to resolve game_id=%s: %s", game_id, exc
            )

        fallback_year = _parse_year_from_date_like(game_id)
        if fallback_year is not None:
            return fallback_year

    return None


def _resolve_target_year(
    payload: "AnalyzeRequest", pool: ConnectionPool
) -> tuple[int, str]:
    league_context = payload.league_context or {}

    if "season_year" in league_context:
        parsed_year = _parse_explicit_year(league_context.get("season_year"))
        if parsed_year is None or not _is_valid_analysis_year(parsed_year):
            raise HTTPException(
                status_code=400,
                detail="invalid_season_year_for_analysis",
            )
        return parsed_year, "league_context.season_year"

    season_id = league_context.get("season")
    season_year = _resolve_year_from_season_id(pool, season_id)
    if season_year is not None and _is_valid_analysis_year(season_year):
        return season_year, "league_context.season->kbo_seasons"

    game_year = _resolve_year_from_game_context(
        pool,
        payload.game_id,
        league_context.get("game_date"),
    )
    if game_year is not None and _is_valid_analysis_year(game_year):
        return game_year, "game_date"

    raise HTTPException(
        status_code=400,
        detail="unable_to_resolve_analysis_year",
    )


def _build_focus_section_requirements(resolved_focus: List[str]) -> str:
    """
    ì„ íƒ focusì— í•´ë‹¹í•˜ëŠ” ìƒì„¸ ì„¹ì…˜ ì œëª© ìš”êµ¬ì‚¬í•­ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not resolved_focus:
        return (
            "- ì„ íƒ focusê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì¢…í•© ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.\n"
            "- ë‹¤ë§Œ detailed_markdownì€ ìµœì†Œ 2ê°œ ì´ìƒì˜ ì†Œì œëª©(##)ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”."
        )

    header_lines = [
        f"- ë°˜ë“œì‹œ `{FOCUS_SECTION_HEADERS[focus]}` ì œëª©ì„ í¬í•¨í•˜ì„¸ìš”."
        for focus in resolved_focus
        if focus in FOCUS_SECTION_HEADERS
    ]
    non_selected = [
        header
        for key, header in FOCUS_SECTION_HEADERS.items()
        if key not in resolved_focus
    ]
    omit_lines = [
        f"- ë¯¸ì„ íƒ focusëŠ” ê°€ëŠ¥í•˜ë©´ ìƒëµí•˜ì„¸ìš”: `{header}`" for header in non_selected
    ]
    return "\n".join(header_lines + omit_lines)


def _find_missing_focus_sections(
    response_data: Dict[str, Any], resolved_focus: List[str]
) -> List[str]:
    """
    detailed_markdownì—ì„œ ì„ íƒ focus ì„¹ì…˜ ëˆ„ë½ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    if not resolved_focus:
        return []

    markdown = str(response_data.get("detailed_markdown") or "")
    missing: List[str] = []
    for focus in resolved_focus:
        header = FOCUS_SECTION_HEADERS.get(focus)
        if not header:
            continue
        if header not in markdown:
            missing.append(focus)
    return missing


router = APIRouter(prefix="/coach", tags=["coach"])


# ============================================================
# Fast Path Helper Functions
# ============================================================


def _build_coach_query(
    team_name: str,
    focus: List[str],
    opponent_name: Optional[str] = None,
    league_context: Optional[Dict[str, Any]] = None,
) -> str:
    """focus ì˜ì—­ì— ë”°ë¼ Coach ì§ˆë¬¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤."""
    focus_text = ", ".join(focus) if focus else "ì¢…í•©ì ì¸ ì „ë ¥"

    if opponent_name:
        query = f"{team_name}ì™€ {opponent_name}ì˜ {focus_text}ì— ëŒ€í•´ ëƒ‰ì² í•˜ê³  ë‹¤ê°ì ì¸ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•´ì¤˜."
    else:
        query = f"{team_name}ì˜ {focus_text}ì— ëŒ€í•´ ëƒ‰ì² í•˜ê³  ë‹¤ê°ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•´ì¤˜."

    # ë¦¬ê·¸ ì»¨í…ìŠ¤íŠ¸ ë°˜ì˜
    if league_context:
        season = league_context.get("season")
        league_type = league_context.get("league_type")
        if league_type == "POST":
            round_name = league_context.get("round", "í¬ìŠ¤íŠ¸ì‹œì¦Œ")
            game_no = league_context.get("game_no")
            query += (
                f" íŠ¹íˆ {season}ë…„ {round_name} {game_no}ì°¨ì „ì„ì„ ê°ì•ˆí•˜ì—¬ ë¶„ì„í•´ì¤˜."
            )
        elif league_type == "REGULAR":
            home_ctx = league_context.get("home", {})
            away_ctx = league_context.get("away", {})
            if home_ctx and away_ctx:
                home_rank = home_ctx.get("rank")
                away_rank = away_ctx.get("rank")
                if home_rank is not None and away_rank is not None:
                    rank_diff = abs(int(home_rank) - int(away_rank))
                    if rank_diff <= 2:
                        query += " ë‘ íŒ€ì˜ ìˆœìœ„ ê²½ìŸì´ ì¹˜ì—´í•œ ìƒí™©ì´ì•¼."

    if "batting" in focus or not focus:
        if opponent_name:
            query += " ì–‘ íŒ€ì˜ íƒ€ê²© ìƒì‚°ì„±(OPS, wRC+)ê³¼ ì£¼ìš” íƒ€ìë“¤ì˜ ìµœê·¼ í´ëŸ¬ì¹˜ ëŠ¥ë ¥ì„ ì§„ë‹¨í•´ì¤˜."
        else:
            query += (
                " íƒ€ê²© ìƒì‚°ì„±(OPS, wRC+)ê³¼ ì£¼ìš” íƒ€ìë“¤ì˜ ìµœê·¼ í´ëŸ¬ì¹˜ ëŠ¥ë ¥ì„ ì§„ë‹¨í•´ì¤˜."
            )

    if "bullpen" in focus:
        query += " ë¶ˆíœì§„ì˜ í•˜ì´ ë ˆë²„ë¦¬ì§€ ìƒí™© ì²˜ë¦¬ ëŠ¥ë ¥ê³¼ ê³¼ë¶€í•˜ ì§€í‘œë¥¼ ë¶„ì„í•´ì¤˜."

    if "recent_form" in focus or not focus:
        query += " ìµœê·¼ 10ê²½ê¸° ìŠ¹íŒ¨ íŠ¸ë Œë“œì™€ ë“ì‹¤ì  ë§ˆì§„ì„ ë³´ê³  íŒ€ì˜ ìƒìŠ¹ì„¸/í•˜ë½ì„¸ë¥¼ ì§„ë‹¨í•´ì¤˜."

    if "starter" in focus:
        query += " ì„ ë°œ ë¡œí…Œì´ì…˜ì˜ ì´ë‹ ì†Œí™”ë ¥ê³¼ QS ë¹„ìœ¨, êµ¬ì† ë³€í™”ë¥¼ ë¶„ì„í•´ì¤˜."

    if "matchup" in focus:
        query += " ì£¼ìš” ë¼ì´ë²Œ íŒ€ë“¤ê³¼ì˜ ìƒëŒ€ ì „ì (ìŠ¹ë¥ , ë“ì‹¤ ë“±)ì„ ë¹„êµ ë¶„ì„í•´ì¤˜."

    return query


async def _execute_coach_tools_parallel(
    pool: ConnectionPool,
    home_team_id: str,
    year: int,
    focus: List[str],
    away_team_id: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Coachì— í•„ìš”í•œ ë„êµ¬ë“¤ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    í™ˆíŒ€ê³¼ ì›ì •íŒ€ ë°ì´í„°ë¥¼ ëª¨ë‘ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    loop = asyncio.get_event_loop()

    def get_team_data(team_code: str):
        """íŠ¹ì • íŒ€ì˜ ëª¨ë“  ë°ì´í„° ì¡°íšŒ"""
        results = {}
        with pool.connection() as conn:
            db_query = DatabaseQueryTool(conn)
            results["summary"] = db_query.get_team_summary(team_code, year)
            results["advanced"] = db_query.get_team_advanced_metrics(team_code, year)
            if "recent_form" in focus or not focus:
                results["recent"] = db_query.get_team_recent_form(team_code, year)
            if "matchup" in focus and away_team_id:
                # ìƒëŒ€ ì „ì ì€ í™ˆíŒ€ ê¸°ì¤€ í•œë²ˆë§Œ ì¡°íšŒí•´ë„ ë¨
                pass
        return results

    def get_matchup_stats_sync(team1: str, team2: str):
        with pool.connection() as conn:
            from app.tools.game_query import GameQueryTool

            game_query = GameQueryTool(conn)
            return game_query.get_head_to_head(team1, team2, year)

    # ë³‘ë ¬ ì‹¤í–‰ íƒœìŠ¤í¬ ì¤€ë¹„
    tasks = []

    # 1. í™ˆíŒ€ ë°ì´í„°
    tasks.append(loop.run_in_executor(None, get_team_data, home_team_id))

    # 2. ì›ì •íŒ€ ë°ì´í„° (ìˆì„ ê²½ìš°)
    if away_team_id:
        tasks.append(loop.run_in_executor(None, get_team_data, away_team_id))

    # 3. ìƒëŒ€ ì „ì  (Matchup focusì¼ ê²½ìš°)
    if "matchup" in focus and away_team_id:
        tasks.append(
            loop.run_in_executor(
                None, get_matchup_stats_sync, home_team_id, away_team_id
            )
        )

    results = await asyncio.gather(*tasks, return_exceptions=True)

    tool_results = {"home": {}, "away": {}, "matchup": {}, "error": None}

    # í™ˆíŒ€ ê²°ê³¼ ì²˜ë¦¬
    if isinstance(results[0], Exception):
        tool_results["error"] = str(results[0])
        tool_results["home"] = {"error": str(results[0])}
    else:
        tool_results["home"] = results[0]

    # ì›ì •íŒ€ ê²°ê³¼ ì²˜ë¦¬
    if away_team_id:
        if isinstance(results[1], Exception):
            tool_results["away"] = {"error": str(results[1])}
        else:
            tool_results["away"] = results[1]

        # ìƒëŒ€ ì „ì  ì²˜ë¦¬
        if "matchup" in focus:
            if len(tasks) > 2 and isinstance(results[2], Exception):
                tool_results["matchup"] = {"error": str(results[2])}
            elif len(tasks) > 2:
                tool_results["matchup"] = results[2]

    # ë ˆê±°ì‹œ êµ¬ì¡° í˜¸í™˜ì„± ìœ ì§€ (ë‹¨ì¼ íŒ€ ë¶„ì„ ìš”ì²­ ì‹œ)
    if not away_team_id:
        tool_results["team_summary"] = tool_results["home"].get("summary", {})
        tool_results["advanced_metrics"] = tool_results["home"].get("advanced", {})
        tool_results["recent_form"] = tool_results["home"].get("recent", {})

    return tool_results


def _safe_float(value: Any, default: float = 0.0) -> float:
    """None-safe float conversion for formatting."""
    if value is None:
        return default
    try:
        return float(value)
    except (ValueError, TypeError):
        return default


def _safe_int(value: Any, default: int = 0) -> int:
    """None-safe int conversion for formatting."""
    if value is None:
        return default
    try:
        return int(value)
    except (ValueError, TypeError):
        return default


def _remove_duplicate_json_start(text: str) -> str:
    """
    LLM ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ë°œìƒí•˜ëŠ” JSON ì‹œì‘ ë¶€ë¶„ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.

    ì¼ë¶€ LLMì€ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ "content restart" í˜„ìƒìœ¼ë¡œ ë™ì¼í•œ í•„ë“œë¥¼
    ë‘ ë²ˆ ì¶œë ¥í•˜ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì¤‘ë³µì„ ê°ì§€í•˜ê³  ì œê±°í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
        ì…ë ¥: '{"headline": "A",\\n"headline": "A", "sentiment": ...'
        ì¶œë ¥: '{"headline": "A", "sentiment": ...'

    Args:
        text: LLMì˜ ì›ì‹œ ì¶œë ¥ í…ìŠ¤íŠ¸

    Returns:
        ì¤‘ë³µì´ ì œê±°ëœ í…ìŠ¤íŠ¸
    """
    import re

    if not text or "{" not in text:
        return text

    # headline í•„ë“œ íŒ¨í„´ ì°¾ê¸°
    headline_pattern = r'"headline"\s*:\s*"[^"]*"'
    matches = list(re.finditer(headline_pattern, text))

    if len(matches) < 2:
        return text

    # ë‘ ê°œ ì´ìƒì˜ headline í•„ë“œê°€ ìˆìœ¼ë©´ ì¤‘ë³µ
    first_match = matches[0]
    second_match = matches[1]

    # ë‘ headline ê°’ì´ ë™ì¼í•œì§€ í™•ì¸
    first_value = text[first_match.start() : first_match.end()]
    second_value = text[second_match.start() : second_match.end()]

    if first_value == second_value:
        # ì¤‘ë³µ ë°œê²¬ - ë‘ ë²ˆì§¸ headline ì´í›„ ë‚´ìš©ë§Œ ìœ ì§€
        logger.warning(
            "[Coach] Duplicate JSON start detected, removing first occurrence"
        )

        # { + ë‘ ë²ˆì§¸ headlineë¶€í„°ì˜ ë‚´ìš©
        brace_pos = text.index("{")
        clean_text = text[brace_pos : brace_pos + 1]  # '{'

        # ë‘ ë²ˆì§¸ headline ì´í›„ ë‚´ìš©
        after_second = text[second_match.start() :]
        clean_text += after_second

        return clean_text

    return text


def _format_team_stats(team_data: Dict[str, Any], team_role: str = "Home") -> str:
    """ë‹¨ì¼ íŒ€ í†µê³„ í¬ë§·íŒ… í—¬í¼"""
    parts = []

    summary = team_data.get("summary", {})
    advanced = team_data.get("advanced", {})
    team_name = summary.get("team_name", "Unknown")

    parts.append(f"### [{team_role}] {team_name}")

    # í•µì‹¬ ì§€í‘œ
    if advanced.get("metrics"):
        batting = advanced["metrics"].get("batting", {})
        pitching = advanced["metrics"].get("pitching", {})
        rankings = advanced.get("rankings", {})

        parts.append("| ì§€í‘œ | ìˆ˜ì¹˜ | ìˆœìœ„ |")
        parts.append("|------|------|------|")
        if batting.get("ops"):
            parts.append(
                f"| OPS | {_safe_float(batting['ops']):.3f} | {rankings.get('batting_ops', '-')}|"
            )
        if pitching.get("avg_era"):
            parts.append(
                f"| ERA | {_safe_float(pitching['avg_era']):.2f} | {pitching.get('era_rank', '-')}|"
            )
        parts.append("")

    # ë¶ˆíœ
    fatigue = advanced.get("fatigue_index", {})
    if fatigue:
        parts.append(f"- **ë¶ˆíœ ë¹„ì¤‘**: {fatigue.get('bullpen_share', '-')}")
        parts.append(f"- **í”¼ë¡œë„ ìˆœìœ„**: {fatigue.get('bullpen_load_rank', '-')}")
        parts.append("")

    # ì£¼ìš” ì„ ìˆ˜ (ê°„ëµí™”)
    top_batters = summary.get("top_batters", [])[:3]
    if top_batters:
        parts.append("**ì£¼ìš” íƒ€ì**:")
        for b in top_batters:
            parts.append(
                f"- {b['player_name']}: OPS {_safe_float(b.get('ops')):.3f}, {b.get('home_runs')}HR"
            )

    top_pitchers = summary.get("top_pitchers", [])[:3]
    if top_pitchers:
        parts.append("**ì£¼ìš” íˆ¬ìˆ˜**:")
        for p in top_pitchers:
            parts.append(
                f"- {p['player_name']}: ERA {_safe_float(p.get('era')):.2f}, {p.get('wins')}ìŠ¹"
            )

    # ìµœê·¼ í¼ â€” DB schema: summary={wins,losses,draws,run_diff}, games=[{result:"Win"/"Loss"/"Draw", score:"5:3", run_diff, date, opponent}]
    recent = team_data.get("recent", {})
    if recent and recent.get("found"):
        parts.append("**ìµœê·¼ ê²½ê¸° íë¦„**:")
        r_summary = recent.get("summary", {})
        r_games = recent.get("games", [])
        wins = r_summary.get("wins", 0)
        losses = r_summary.get("losses", 0)
        draws = r_summary.get("draws", 0)
        parts.append(
            f"- ìµœê·¼ {len(r_games)}ê²½ê¸°: {wins}ìŠ¹ {losses}íŒ¨{f' {draws}ë¬´' if draws else ''}"
        )
        run_diff = r_summary.get("run_diff")
        if run_diff is not None:
            parts.append(f"- ë“ì‹¤ ë§ˆì§„: {'+' if run_diff >= 0 else ''}{run_diff}")
        win_rate = r_summary.get("win_rate")
        if win_rate is not None:
            parts.append(f"- ìŠ¹ë¥ : {win_rate:.3f}")
        parts.append("")

    parts.append("")
    return "\n".join(parts)


def _format_coach_context(
    tool_results: Dict[str, Any],
    focus: List[str],
    game_context: Optional[str] = None,
    league_context: Optional[Dict[str, Any]] = None,
) -> str:
    """
    Coach ì „ìš© ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬ë§·í•©ë‹ˆë‹¤.
    ë“€ì–¼ íŒ€ ë°ì´í„° ì§€ì›.
    """
    parts = []

    # 1. ë¦¬ê·¸/ê²½ê¸° ì»¨í…ìŠ¤íŠ¸
    if league_context:
        season = league_context.get("season")
        league_type = league_context.get("league_type")
        parts.append(f"## ğŸŸï¸ {season} ì‹œì¦Œ ì»¨í…ìŠ¤íŠ¸")

        if league_type == "POST":
            parts.append(
                f"**{league_context.get('round')} {league_context.get('game_no')}ì°¨ì „**"
            )
        else:
            home = league_context.get("home", {})
            away = league_context.get("away", {})
            parts.append(
                f"- **Home**: {home.get('rank')}ìœ„ ({home.get('gamesBehind')} GB)"
            )
            parts.append(
                f"- **Away**: {away.get('rank')}ìœ„ ({away.get('gamesBehind')} GB)"
            )
        parts.append("")

    # 2. ê²½ê¸° ë³„ ëª¨ë“œ ì•ˆë‚´
    if game_context:
        parts.append("## âš ï¸ íŠ¹ì • ê²½ê¸° ë¶„ì„ ëª¨ë“œ")
        parts.append(f"**ë¶„ì„ ëŒ€ìƒ**: {game_context}")
        parts.append("")

    # 3. íŒ€ë³„ ë°ì´í„°
    if tool_results.get("home"):
        parts.append(_format_team_stats(tool_results["home"], "Home"))

    if tool_results.get("away"):
        parts.append(_format_team_stats(tool_results["away"], "Away"))

    # 4. ìƒëŒ€ ì „ì 
    matchup = tool_results.get("matchup", {})
    if matchup and matchup.get("games"):
        parts.append("### âš”ï¸ ë§ëŒ€ê²° ì „ì ")
        summary = matchup.get("summary", {})
        t1 = matchup.get("team1", "íŒ€1")
        t2 = matchup.get("team2", "íŒ€2")
        parts.append(
            f"- {t1} {summary.get('team1_wins', 0)}ìŠ¹ / "
            f"{t2} {summary.get('team2_wins', 0)}ìŠ¹ / "
            f"{summary.get('draws', 0)}ë¬´"
        )
        parts.append("| ë‚ ì§œ | ìŠ¤ì½”ì–´ | ê²°ê³¼ |")
        parts.append("|------|--------|------|")
        for g in matchup.get("games", [])[:3]:
            game_date = g.get("game_date", "")
            if hasattr(game_date, "strftime"):
                game_date = game_date.strftime("%Y-%m-%d")
            score = f"{g.get('home_score', 0)}:{g.get('away_score', 0)}"
            result_val = g.get("game_result", "")
            parts.append(f"| {game_date} | {score} | {result_val} |")
        parts.append("")

    return "\n".join(parts)


class AnalyzeRequest(BaseModel):
    team_id: Optional[str] = None  # deprecated â€” use home_team_id
    home_team_id: Optional[str] = None
    away_team_id: Optional[str] = None
    league_context: Optional[Dict[str, Any]] = None
    focus: List[str] = []
    game_id: Optional[str] = None
    request_mode: Literal["auto_brief", "manual_detail"] = "manual_detail"
    question_override: Optional[str] = None

    @model_validator(mode="before")
    @classmethod
    def backfill_home_team_id(cls, values: Any) -> Any:
        """team_idë§Œ ë³´ë‚´ëŠ” ê¸°ì¡´ í˜¸ì¶œì„ home_team_idë¡œ ë§¤í•‘"""
        if isinstance(values, dict):
            if not values.get("home_team_id") and values.get("team_id"):
                values["home_team_id"] = values["team_id"]
        return values


@router.post("/analyze")
async def analyze_team(
    payload: AnalyzeRequest,
    agent: BaseballStatisticsAgent = Depends(get_agent),
    _: None = Depends(rate_limit_dependency),
):
    """
    íŠ¹ì • íŒ€(ë“¤)ì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ì„ ìš”ì²­í•©ë‹ˆë‹¤. 'The Coach' í˜ë¥´ì†Œë‚˜ê°€ ì ìš©ë©ë‹ˆë‹¤.
    """
    from sse_starlette.sse import EventSourceResponse

    # í•˜ìœ„ í˜¸í™˜ì„±ì€ model_validatorì—ì„œ ì²˜ë¦¬ë¨
    if not payload.home_team_id:
        raise HTTPException(
            status_code=400, detail="home_team_id ë˜ëŠ” team_idê°€ í•„ìš”í•©ë‹ˆë‹¤."
        )

    try:
        request_id = uuid.uuid4().hex[:8]
        pool = get_connection_pool()
        team_resolver = TeamCodeResolver()

        home_team_canonical = team_resolver.resolve_canonical(payload.home_team_id)
        away_team_canonical = (
            team_resolver.resolve_canonical(payload.away_team_id)
            if payload.away_team_id
            else None
        )
        if home_team_canonical not in CANONICAL_CODES:
            raise HTTPException(
                status_code=400,
                detail="unsupported_team_for_regular_analysis",
            )
        if away_team_canonical and away_team_canonical not in CANONICAL_CODES:
            raise HTTPException(
                status_code=400,
                detail="unsupported_team_for_regular_analysis",
            )

        home_name = agent._convert_team_id_to_name(payload.home_team_id)
        away_name = (
            agent._convert_team_id_to_name(payload.away_team_id)
            if payload.away_team_id
            else None
        )
        request_mode = payload.request_mode
        is_auto_brief = request_mode == "auto_brief"
        input_focus = list(payload.focus or [])
        resolved_focus = (
            ["recent_form"] if is_auto_brief else normalize_focus(input_focus)
        )
        if is_auto_brief:
            if payload.question_override:
                logger.warning(
                    "[Coach Router] auto_brief ignores question_override for %s: %s",
                    home_name,
                    payload.question_override,
                )
            effective_question_override = None
            question_signature_override = "auto"
            query = _build_coach_query(
                home_name,
                resolved_focus,
                opponent_name=away_name,
                league_context=payload.league_context,
            )
        else:
            effective_question_override = payload.question_override
            question_signature_override = None
            if payload.question_override:
                query = payload.question_override
            else:
                query = _build_coach_query(
                    home_name,
                    resolved_focus,
                    opponent_name=away_name,
                    league_context=payload.league_context,
                )

        year, resolve_source = _resolve_target_year(payload, pool)
        if not _is_valid_analysis_year(year):
            raise HTTPException(status_code=400, detail="analysis_year_out_of_range")

        settings = get_settings()
        coach_model_name = settings.coach_openrouter_model or settings.openrouter_model

        # Cache Key ìƒì„±
        game_type = str(
            (payload.league_context or {}).get("league_type") or "UNKNOWN"
        ).upper()
        cache_key, cache_key_payload = build_coach_cache_key(
            schema_version=COACH_CACHE_SCHEMA_VERSION,
            prompt_version=COACH_CACHE_PROMPT_VERSION,
            home_team_code=home_team_canonical,
            away_team_code=away_team_canonical,
            year=year,
            game_type=game_type,
            focus=resolved_focus,
            question_override=effective_question_override,
            question_signature_override=question_signature_override,
        )
        focus_signature = str(cache_key_payload["focus_signature"])
        question_signature = str(cache_key_payload["question_signature"])

        logger.info(
            "[Coach Router] request_mode=%s Analyzing %s vs %s (year=%d): %s... (CacheKey: %s) input_season=%s resolved_year=%d resolve_source=%s input_focus=%s resolved_focus=%s focus_signature=%s question_signature=%s cache_key_version=%s",
            request_mode,
            home_name,
            away_name or "Single",
            year,
            query[:100],
            cache_key,
            (payload.league_context or {}).get("season"),
            year,
            resolve_source,
            input_focus,
            resolved_focus,
            focus_signature,
            question_signature,
            COACH_CACHE_SCHEMA_VERSION,
        )

        async def event_generator():
            try:
                total_start = perf_counter()

                # Phase 1: ì‹œì‘
                yield {
                    "event": "status",
                    "data": json.dumps(
                        {"message": "ì–‘ íŒ€ ì „ë ¥ ë¶„ì„ ì¤‘..."}, ensure_ascii=False
                    ),
                }
                # Phase 0: ìºì‹œ í™•ì¸
                cached_data = None
                cache_state = "MISS_GENERATE"
                cache_error_message = None

                with pool.connection() as conn:
                    with conn.transaction():
                        inserted = conn.execute(
                            """
                            INSERT INTO coach_analysis_cache (
                                cache_key, team_id, year, prompt_version, model_name, status, error_message, updated_at
                            ) VALUES (%s, %s, %s, %s, %s, 'PENDING', NULL, now())
                            ON CONFLICT (cache_key) DO NOTHING
                            RETURNING cache_key
                            """,
                            (
                                cache_key,
                                home_team_canonical,
                                year,
                                COACH_CACHE_PROMPT_VERSION,
                                coach_model_name,
                            ),
                        ).fetchone()
                        row = conn.execute(
                            """
                            SELECT status, response_json, error_message, updated_at
                            FROM coach_analysis_cache
                            WHERE cache_key = %s
                            FOR UPDATE
                            """,
                            (cache_key,),
                        ).fetchone()

                        if inserted:
                            cache_state = "MISS_GENERATE"
                        elif row:
                            status, cached_json, error_message, updated_at = row
                            cache_state = _determine_cache_gate(
                                status=status,
                                has_cached_json=bool(cached_json),
                                updated_at=updated_at,
                            )

                            if cache_state == "HIT":
                                cached_data = cached_json
                            elif cache_state in {
                                "MISS_GENERATE",
                                "PENDING_STALE_TAKEOVER",
                            }:
                                conn.execute(
                                    """
                                    UPDATE coach_analysis_cache
                                    SET status = 'PENDING',
                                        team_id = %s,
                                        year = %s,
                                        prompt_version = %s,
                                        model_name = %s,
                                        error_message = NULL,
                                        updated_at = now()
                                    WHERE cache_key = %s
                                    """,
                                    (
                                        home_team_canonical,
                                        year,
                                        COACH_CACHE_PROMPT_VERSION,
                                        coach_model_name,
                                        cache_key,
                                    ),
                                )
                            elif cache_state == "FAILED_LOCKED":
                                cache_error_message = error_message
                        else:
                            # Defensive fallback: row should exist after INSERT/SELECT, but keep service available.
                            cache_state = "MISS_GENERATE"

                logger.info(
                    "[Coach] Cache %s for %s focus_signature=%s question_signature=%s request_mode=%s cache_key_version=%s",
                    cache_state,
                    cache_key,
                    focus_signature,
                    question_signature,
                    request_mode,
                    COACH_CACHE_SCHEMA_VERSION,
                )

                if cached_data:
                    cached_data = _normalize_cached_response(cached_data)
                    missing_focus_sections = _find_missing_focus_sections(
                        cached_data, resolved_focus
                    )
                    yield {
                        "event": "status",
                        "data": json.dumps(
                            {"message": "ë¶„ì„ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤..."},
                            ensure_ascii=False,
                        ),
                    }
                    json_str = json.dumps(cached_data, ensure_ascii=False, indent=2)
                    yield {
                        "event": "message",
                        "data": json.dumps({"delta": json_str}, ensure_ascii=False),
                    }
                    yield {
                        "event": "meta",
                        "data": json.dumps(
                            {
                                "validation_status": "success",
                                "structured_response": cached_data,
                                "fast_path": True,
                                "cached": True,
                                "request_mode": request_mode,
                                "resolved_focus": resolved_focus,
                                "focus_signature": focus_signature,
                                "question_signature": question_signature,
                                "cache_key_version": COACH_CACHE_SCHEMA_VERSION,
                                "cache_state": cache_state,
                                "in_progress": False,
                                "focus_section_missing": bool(missing_focus_sections),
                                "missing_focus_sections": missing_focus_sections,
                            },
                            ensure_ascii=False,
                        ),
                    }
                    yield {"event": "done", "data": "[DONE]"}
                    return

                if not _should_generate_from_gate(cache_state):
                    if cache_state == "PENDING_WAIT":
                        wait_result = await _wait_for_cache_terminal_state(
                            pool=pool,
                            cache_key=cache_key,
                            timeout_seconds=PENDING_WAIT_TIMEOUT_SECONDS,
                            poll_ms=PENDING_WAIT_POLL_MS,
                        )
                        if (
                            wait_result
                            and wait_result.get("status") == "COMPLETED"
                            and wait_result.get("response_json")
                        ):
                            cached_wait_data = _normalize_cached_response(
                                wait_result["response_json"]
                            )
                            missing_focus_sections = _find_missing_focus_sections(
                                cached_wait_data, resolved_focus
                            )
                            yield {
                                "event": "status",
                                "data": json.dumps(
                                    {
                                        "message": "ì§„í–‰ ì¤‘ì´ë˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤..."
                                    },
                                    ensure_ascii=False,
                                ),
                            }
                            json_str = json.dumps(
                                cached_wait_data, ensure_ascii=False, indent=2
                            )
                            yield {
                                "event": "message",
                                "data": json.dumps(
                                    {"delta": json_str}, ensure_ascii=False
                                ),
                            }
                            yield {
                                "event": "meta",
                                "data": json.dumps(
                                    {
                                        "validation_status": "success",
                                        "structured_response": cached_wait_data,
                                        "fast_path": True,
                                        "cached": True,
                                        "request_mode": request_mode,
                                        "resolved_focus": resolved_focus,
                                        "focus_signature": focus_signature,
                                        "question_signature": question_signature,
                                        "cache_key_version": COACH_CACHE_SCHEMA_VERSION,
                                        "cache_state": "PENDING_WAIT",
                                        "in_progress": False,
                                        "focus_section_missing": bool(
                                            missing_focus_sections
                                        ),
                                        "missing_focus_sections": missing_focus_sections,
                                    },
                                    ensure_ascii=False,
                                ),
                            }
                            yield {"event": "done", "data": "[DONE]"}
                            return

                        if wait_result and wait_result.get("status") == "FAILED":
                            cache_state = "FAILED_LOCKED"
                            cache_error_message = (
                                wait_result.get("error_message")
                                or "analysis_generation_failed"
                            )
                        else:
                            waiting_payload = _cache_status_response(
                                headline=f"{home_name} ë¶„ì„ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤",
                                coach_note="ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                                detail="## ìºì‹œ ì¤€ë¹„ ì¤‘\n\në™ì¼ ê²½ê¸° ë¶„ì„ ìš”ì²­ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.",
                            )
                            yield {
                                "event": "status",
                                "data": json.dumps(
                                    {
                                        "message": "ê¸°ì¡´ ë¶„ì„ ì‘ì—…ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘ì…ë‹ˆë‹¤..."
                                    },
                                    ensure_ascii=False,
                                ),
                            }
                            yield {
                                "event": "message",
                                "data": json.dumps(
                                    {
                                        "delta": json.dumps(
                                            waiting_payload, ensure_ascii=False
                                        )
                                    },
                                    ensure_ascii=False,
                                ),
                            }
                            yield {
                                "event": "meta",
                                "data": json.dumps(
                                    {
                                        "validation_status": "fallback",
                                        "fast_path": True,
                                        "cached": False,
                                        "request_mode": request_mode,
                                        "resolved_focus": resolved_focus,
                                        "focus_signature": focus_signature,
                                        "question_signature": question_signature,
                                        "cache_key_version": COACH_CACHE_SCHEMA_VERSION,
                                        "cache_state": "PENDING_WAIT",
                                        "in_progress": True,
                                    },
                                    ensure_ascii=False,
                                ),
                            }
                            yield {"event": "done", "data": "[DONE]"}
                            return

                    if cache_state == "FAILED_LOCKED":
                        failed_payload = _cache_status_response(
                            headline=f"{home_name} ë¶„ì„ ìºì‹œ ê°±ì‹ ì´ í•„ìš”í•©ë‹ˆë‹¤",
                            coach_note="ìˆ˜ë™ ë°°ì¹˜ë¡œ ìºì‹œë¥¼ ê°±ì‹ í•œ ë’¤ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                            detail=(
                                "## ìºì‹œ ì ê¸ˆ ìƒíƒœ\n\n"
                                "ìë™ ì¬ìƒì„±ì€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n\n"
                                f"ì‚¬ìœ : {cache_error_message or 'previous_failure'}"
                            ),
                        )
                        yield {
                            "event": "status",
                            "data": json.dumps(
                                {"message": "ë¶„ì„ ìºì‹œê°€ ì ê¸ˆ ìƒíƒœì…ë‹ˆë‹¤."},
                                ensure_ascii=False,
                            ),
                        }
                        yield {
                            "event": "message",
                            "data": json.dumps(
                                {
                                    "delta": json.dumps(
                                        failed_payload, ensure_ascii=False
                                    )
                                },
                                ensure_ascii=False,
                            ),
                        }
                        yield {
                            "event": "meta",
                            "data": json.dumps(
                                {
                                    "validation_status": "fallback",
                                    "fast_path": True,
                                    "cached": False,
                                    "request_mode": request_mode,
                                    "resolved_focus": resolved_focus,
                                    "focus_signature": focus_signature,
                                    "question_signature": question_signature,
                                    "cache_key_version": COACH_CACHE_SCHEMA_VERSION,
                                    "cache_state": "FAILED_LOCKED",
                                    "in_progress": False,
                                },
                                ensure_ascii=False,
                            ),
                        }
                        yield {"event": "done", "data": "[DONE]"}
                        return

                # ë„êµ¬ ì‹¤í–‰
                yield {
                    "event": "tool_start",
                    "data": json.dumps(
                        {"tool": "parallel_fetch_team_data"}, ensure_ascii=False
                    ),
                }

                tool_results = await _execute_coach_tools_parallel(
                    pool,
                    payload.home_team_id,
                    year,
                    resolved_focus,
                    payload.away_team_id,
                )

                yield {
                    "event": "tool_result",
                    "data": json.dumps(
                        {
                            "tool": "parallel_fetch_team_data",
                            "success": True,
                            "message": "ë°ì´í„° ì¡°íšŒ ì™„ë£Œ",
                        },
                        ensure_ascii=False,
                    ),
                }

                # Phase 2: ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
                game_context = (
                    payload.question_override if payload.question_override else None
                )
                # Game info fetching can be added here if needed, consistent with tool_results usage

                context = _format_coach_context(
                    tool_results, resolved_focus, game_context, payload.league_context
                )

                # ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ (ê°„ì†Œí™”)
                # í™ˆíŒ€ ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
                home_data = tool_results.get("home", {})
                has_home_data = bool(home_data.get("summary")) or bool(
                    home_data.get("advanced")
                )

                if not has_home_data:
                    logger.warning("[Coach] Data validation failed - skipping LLM call")
                    with pool.connection() as conn:
                        conn.execute(
                            "UPDATE coach_analysis_cache SET status = 'FAILED', error_message = %s, updated_at = now() WHERE cache_key = %s",
                            ("Data insufficient", cache_key),
                        )
                        conn.commit()

                    fallback_response = json.dumps(
                        {
                            "headline": f"{home_name} ë°ì´í„°ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                            "sentiment": "neutral",
                            "key_metrics": [],
                            "analysis": {
                                "strengths": [],
                                "weaknesses": [],
                                "risks": [],
                            },
                            "detailed_markdown": "## ë°ì´í„° ë¶€ì¡±\n\në°ì´í„°ë¥¼ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                            "coach_note": "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                        },
                        ensure_ascii=False,
                    )

                    yield {
                        "event": "message",
                        "data": json.dumps(
                            {"delta": fallback_response}, ensure_ascii=False
                        ),
                    }
                    yield {"event": "done", "data": "[DONE]"}
                    return

                # Phase 3: LLM í˜¸ì¶œ
                yield {
                    "event": "status",
                    "data": json.dumps(
                        {"message": "AI ì½”ì¹˜ê°€ ë¶„ì„ ë¦¬í¬íŠ¸ ì‘ì„± ì¤‘..."},
                        ensure_ascii=False,
                    ),
                }

                focus_section_requirements = _build_focus_section_requirements(
                    resolved_focus
                )
                coach_prompt = COACH_PROMPT_V2.format(
                    question=query,
                    context=context,
                    focus_section_requirements=focus_section_requirements,
                )
                messages = [{"role": "user", "content": coach_prompt}]

                coach_llm = get_coach_llm_generator()
                settings = get_settings()
                effective_max_tokens = (
                    settings.coach_brief_max_output_tokens
                    if is_auto_brief
                    else settings.coach_max_output_tokens
                )
                response_chunks = []

                async for chunk in coach_llm(
                    messages=messages,
                    max_tokens=effective_max_tokens,
                ):
                    response_chunks.append(chunk)
                    yield {
                        "event": "message",
                        "data": json.dumps({"delta": chunk}, ensure_ascii=False),
                    }
                full_response = "".join(response_chunks)
                full_response = _remove_duplicate_json_start(full_response)

                # Phase 4: ê²€ì¦ ë° ì €ì¥
                parsed_response, parse_error = parse_coach_response(full_response)
                missing_focus_sections = []
                if parsed_response:
                    missing_focus_sections = _find_missing_focus_sections(
                        parsed_response.model_dump(), resolved_focus
                    )
                    if missing_focus_sections:
                        logger.warning(
                            "[Coach] Missing focus sections detected focus=%s missing=%s cache_key=%s",
                            resolved_focus,
                            missing_focus_sections,
                            cache_key,
                        )

                with pool.connection() as conn:
                    if parsed_response:
                        conn.execute(
                            "UPDATE coach_analysis_cache SET status = 'COMPLETED', response_json = %s, updated_at = now() WHERE cache_key = %s",
                            (
                                json.dumps(
                                    parsed_response.model_dump(), ensure_ascii=False
                                ),
                                cache_key,
                            ),
                        )
                    else:
                        conn.execute(
                            "UPDATE coach_analysis_cache SET status = 'FAILED', error_message = %s, updated_at = now() WHERE cache_key = %s",
                            (parse_error or "Validation failed", cache_key),
                        )
                    conn.commit()

                meta_payload = {
                    "verified": True,
                    "fast_path": True,
                    "validation_status": "success" if parsed_response else "fallback",
                    "request_mode": request_mode,
                    "resolved_focus": resolved_focus,
                    "focus_signature": focus_signature,
                    "question_signature": question_signature,
                    "cache_key_version": COACH_CACHE_SCHEMA_VERSION,
                    "cache_state": cache_state,
                    "in_progress": False,
                    "focus_section_missing": bool(missing_focus_sections),
                    "missing_focus_sections": missing_focus_sections,
                }
                if parsed_response:
                    meta_payload["structured_response"] = parsed_response.model_dump()

                yield {
                    "event": "meta",
                    "data": json.dumps(meta_payload, ensure_ascii=False),
                }

                yield {"event": "done", "data": "[DONE]"}
            except Exception as e:
                logger.error(f"[Coach Streaming Error] {e}")
                # Cache fail logic
                try:
                    fallback_pool = get_connection_pool()
                    with fallback_pool.connection() as conn:
                        conn.execute(
                            "UPDATE coach_analysis_cache SET status = 'FAILED', error_message = %s, updated_at = now() WHERE cache_key = %s",
                            (str(e), cache_key),
                        )
                        conn.commit()
                except:  # noqa: BLE001
                    pass

                yield {
                    "event": "error",
                    "data": json.dumps({"error": str(e)}, ensure_ascii=False),
                }
                yield {"event": "done", "data": "[DONE]"}

        return EventSourceResponse(
            event_generator(),
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no",
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[Coach Router] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================
# Legacy endpoint (ê¸°ì¡´ ë°©ì‹ ìœ ì§€, í•„ìš” ì‹œ ì‚¬ìš©)
# ============================================================


@router.post("/analyze-legacy")
async def analyze_team_legacy(
    payload: AnalyzeRequest,
    agent: BaseballStatisticsAgent = Depends(get_agent),
    _: None = Depends(rate_limit_dependency),
):
    """
    ê¸°ì¡´ ë°©ì‹ì˜ Coach ë¶„ì„ (ì „ì²´ ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©).
    Fast Pathì— ë¬¸ì œê°€ ìˆì„ ê²½ìš° ëŒ€ì•ˆìœ¼ë¡œ ì‚¬ìš©.
    """
    from sse_starlette.sse import EventSourceResponse

    try:
        primary_team_id = payload.home_team_id or payload.team_id
        if not primary_team_id:
            raise HTTPException(
                status_code=400, detail="home_team_id ë˜ëŠ” team_idê°€ í•„ìš”í•©ë‹ˆë‹¤."
            )

        team_name = agent._convert_team_id_to_name(primary_team_id)
        resolved_focus = normalize_focus(payload.focus)

        if payload.question_override:
            query = payload.question_override
        else:
            query = _build_coach_query(team_name, resolved_focus)

        logger.info(f"[Coach Router Legacy] Analyzing for {team_name}")

        context_data = {"persona": "coach", "team_id": primary_team_id}

        async def event_generator():
            try:
                async for event in agent.process_query_stream(
                    query, context=context_data
                ):
                    if event["type"] == "status":
                        yield {
                            "event": "status",
                            "data": json.dumps(
                                {"message": event["message"]}, ensure_ascii=False
                            ),
                        }
                    elif event["type"] == "tool_start":
                        yield {
                            "event": "tool_start",
                            "data": json.dumps(
                                {"tool": event["tool"]}, ensure_ascii=False
                            ),
                        }
                    elif event["type"] == "tool_result":
                        yield {
                            "event": "tool_result",
                            "data": json.dumps(
                                {
                                    "tool": event["tool"],
                                    "success": event["success"],
                                    "message": event["message"],
                                },
                                ensure_ascii=False,
                            ),
                        }
                    elif event["type"] == "answer_chunk":
                        yield {
                            "event": "message",
                            "data": json.dumps(
                                {"delta": event["content"]}, ensure_ascii=False
                            ),
                        }
                    elif event["type"] == "metadata":
                        meta_payload = {
                            "tool_calls": [
                                tc.to_dict() for tc in event["data"]["tool_calls"]
                            ],
                            "verified": event["data"]["verified"],
                            "data_sources": event["data"]["data_sources"],
                        }
                        yield {
                            "event": "meta",
                            "data": json.dumps(meta_payload, ensure_ascii=False),
                        }

                yield {"event": "done", "data": "[DONE]"}
            except Exception as e:
                logger.error(f"[Coach Legacy Streaming Error] {e}")
                import traceback

                logger.error(traceback.format_exc())
                yield {
                    "event": "error",
                    "data": json.dumps({"error": str(e)}, ensure_ascii=False),
                }
                yield {"event": "done", "data": "[DONE]"}

        return EventSourceResponse(
            event_generator(),
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no",
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[Coach Router Legacy] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
