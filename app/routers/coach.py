"""
'The Coach' ê¸°ëŠ¥ê³¼ ê´€ë ¨ëœ API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

Fast Path ìµœì í™”:
- ë„êµ¬ ê³„íš LLM í˜¸ì¶œì„ ê±´ë„ˆë›°ê³  focus ì˜ì—­ì— ë”°ë¼ ì§ì ‘ ë„êµ¬ í˜¸ì¶œ
- ë³‘ë ¬ ë„êµ¬ ì‹¤í–‰ìœ¼ë¡œ ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶•
- Coach ì „ìš© ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
"""

import logging
import json
import asyncio
import uuid
import hashlib
from time import perf_counter
from datetime import datetime
from typing import List, Optional, Dict, Any
from fastapi import APIRouter, Body, Depends, HTTPException
from pydantic import BaseModel, model_validator

from psycopg_pool import ConnectionPool

from ..deps import (
    get_agent,
    get_db_connection,
    get_connection_pool,
    get_coach_llm_generator,
)
from ..agents.baseball_agent import BaseballStatisticsAgent
from ..core.prompts import COACH_PROMPT_V2
from ..core.coach_validator import (
    parse_coach_response,
    CoachResponse,
    _create_fallback_response,
)
from ..core.ratelimit import rate_limit_dependency
from ..tools.database_query import DatabaseQueryTool

logger = logging.getLogger(__name__)

# ë¹ˆ ì‘ë‹µ ì‹œ ì¬ì‹œë„ íšŸìˆ˜
MAX_RETRY_ON_EMPTY = 2


def _normalize_cached_response(cached_data: dict) -> dict:
    """
    ë ˆê±°ì‹œ ìºì‹œ ë°ì´í„°ë¥¼ í˜„ì¬ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì •ê·œí™”í•©ë‹ˆë‹¤.

    CoachResponse ê²€ì¦ê¸°ë¥¼ í†µê³¼ì‹œì¼œ ìë™ìœ¼ë¡œ ë³€í™˜:
    - status: "ì£¼ì˜" â†’ "warning", "ì–‘í˜¸" â†’ "good", "ìœ„í—˜" â†’ "danger"
    - area: "ë¶ˆíœ" â†’ "bullpen", "ì„ ë°œ" â†’ "starter", "íƒ€ê²©" â†’ "batting"
    - coach_note: 150ì ì´ˆê³¼ ì‹œ ìë™ truncate

    Args:
        cached_data: ìºì‹œì—ì„œ ì½ì€ ì›ë³¸ JSON ë°ì´í„°

    Returns:
        ì •ê·œí™”ëœ ë°ì´í„° (ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜)
    """
    if not cached_data:
        return cached_data

    try:
        # CoachResponse ê²€ì¦ê¸°ë¥¼ í†µê³¼ì‹œì¼œ ìë™ ì •ê·œí™”
        response = CoachResponse(**cached_data)
        normalized = response.model_dump()
        logger.debug("[Coach Cache] Normalized legacy data")
        return normalized
    except Exception as e:
        logger.warning(f"[Coach Cache] Failed to normalize legacy data: {e}")
        return cached_data


router = APIRouter(prefix="/coach", tags=["coach"])


# ============================================================
# Fast Path Helper Functions
# ============================================================


def _build_coach_query(
    team_name: str,
    focus: List[str],
    opponent_name: Optional[str] = None,
    league_context: Optional[Dict[str, Any]] = None,
) -> str:
    """focus ì˜ì—­ì— ë”°ë¼ Coach ì§ˆë¬¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤."""
    focus_text = ", ".join(focus) if focus else "ì¢…í•©ì ì¸ ì „ë ¥"

    if opponent_name:
        query = f"{team_name}ì™€ {opponent_name}ì˜ {focus_text}ì— ëŒ€í•´ ëƒ‰ì² í•˜ê³  ë‹¤ê°ì ì¸ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•´ì¤˜."
    else:
        query = (
            f"{team_name}ì˜ {focus_text}ì— ëŒ€í•´ ëƒ‰ì² í•˜ê³  ë‹¤ê°ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•´ì¤˜."
        )

    # ë¦¬ê·¸ ì»¨í…ìŠ¤íŠ¸ ë°˜ì˜
    if league_context:
        season = league_context.get("season")
        league_type = league_context.get("league_type")
        if league_type == "POST":
            round_name = league_context.get("round", "í¬ìŠ¤íŠ¸ì‹œì¦Œ")
            game_no = league_context.get("game_no")
            query += f" íŠ¹íˆ {season}ë…„ {round_name} {game_no}ì°¨ì „ì„ì„ ê°ì•ˆí•˜ì—¬ ë¶„ì„í•´ì¤˜."
        elif league_type == "REGULAR":
            home_ctx = league_context.get("home", {})
            away_ctx = league_context.get("away", {})
            if home_ctx and away_ctx:
                home_rank = home_ctx.get("rank")
                away_rank = away_ctx.get("rank")
                if home_rank is not None and away_rank is not None:
                    rank_diff = abs(int(home_rank) - int(away_rank))
                    if rank_diff <= 2:
                        query += " ë‘ íŒ€ì˜ ìˆœìœ„ ê²½ìŸì´ ì¹˜ì—´í•œ ìƒí™©ì´ì•¼."

    if "batting" in focus or not focus:
        if opponent_name:
            query += (
                " ì–‘ íŒ€ì˜ íƒ€ê²© ìƒì‚°ì„±(OPS, wRC+)ê³¼ ì£¼ìš” íƒ€ìë“¤ì˜ ìµœê·¼ í´ëŸ¬ì¹˜ ëŠ¥ë ¥ì„ ì§„ë‹¨í•´ì¤˜."
            )
        else:
            query += (
                " íƒ€ê²© ìƒì‚°ì„±(OPS, wRC+)ê³¼ ì£¼ìš” íƒ€ìë“¤ì˜ ìµœê·¼ í´ëŸ¬ì¹˜ ëŠ¥ë ¥ì„ ì§„ë‹¨í•´ì¤˜."
            )

    if "bullpen" in focus:
        query += " ë¶ˆíœì§„ì˜ í•˜ì´ ë ˆë²„ë¦¬ì§€ ìƒí™© ì²˜ë¦¬ ëŠ¥ë ¥ê³¼ ê³¼ë¶€í•˜ ì§€í‘œë¥¼ ë¶„ì„í•´ì¤˜."

    if "recent_form" in focus or not focus:
        query += " ìµœê·¼ 10ê²½ê¸° ìŠ¹íŒ¨ íŠ¸ë Œë“œì™€ ë“ì‹¤ì  ë§ˆì§„ì„ ë³´ê³  íŒ€ì˜ ìƒìŠ¹ì„¸/í•˜ë½ì„¸ë¥¼ ì§„ë‹¨í•´ì¤˜."

    if "starter" in focus:
        query += " ì„ ë°œ ë¡œí…Œì´ì…˜ì˜ ì´ë‹ ì†Œí™”ë ¥ê³¼ QS ë¹„ìœ¨, êµ¬ì† ë³€í™”ë¥¼ ë¶„ì„í•´ì¤˜."

    if "matchup" in focus:
        query += " ì£¼ìš” ë¼ì´ë²Œ íŒ€ë“¤ê³¼ì˜ ìƒëŒ€ ì „ì (ìŠ¹ë¥ , ë“ì‹¤ ë“±)ì„ ë¹„êµ ë¶„ì„í•´ì¤˜."

    return query


async def _execute_coach_tools_parallel(
    pool: ConnectionPool,
    home_team_id: str,
    year: int,
    focus: List[str],
    away_team_id: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Coachì— í•„ìš”í•œ ë„êµ¬ë“¤ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    í™ˆíŒ€ê³¼ ì›ì •íŒ€ ë°ì´í„°ë¥¼ ëª¨ë‘ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    loop = asyncio.get_event_loop()

    def get_team_data(team_code: str):
        """íŠ¹ì • íŒ€ì˜ ëª¨ë“  ë°ì´í„° ì¡°íšŒ"""
        results = {}
        with pool.connection() as conn:
            db_query = DatabaseQueryTool(conn)
            results["summary"] = db_query.get_team_summary(team_code, year)
            results["advanced"] = db_query.get_team_advanced_metrics(team_code, year)
            if "recent_form" in focus or not focus:
                results["recent"] = db_query.get_team_recent_form(team_code, year)
            if "matchup" in focus and away_team_id:
                # ìƒëŒ€ ì „ì ì€ í™ˆíŒ€ ê¸°ì¤€ í•œë²ˆë§Œ ì¡°íšŒí•´ë„ ë¨
                pass
        return results

    def get_matchup_stats_sync(team1: str, team2: str):
        with pool.connection() as conn:
            from app.tools.game_query import GameQueryTool
            game_query = GameQueryTool(conn)
            return game_query.get_head_to_head(team1, team2, year)

    # ë³‘ë ¬ ì‹¤í–‰ íƒœìŠ¤í¬ ì¤€ë¹„
    tasks = []
    
    # 1. í™ˆíŒ€ ë°ì´í„°
    tasks.append(loop.run_in_executor(None, get_team_data, home_team_id))
    
    # 2. ì›ì •íŒ€ ë°ì´í„° (ìˆì„ ê²½ìš°)
    if away_team_id:
        tasks.append(loop.run_in_executor(None, get_team_data, away_team_id))
        
    # 3. ìƒëŒ€ ì „ì  (Matchup focusì¼ ê²½ìš°)
    if "matchup" in focus and away_team_id:
        tasks.append(loop.run_in_executor(None, get_matchup_stats_sync, home_team_id, away_team_id))

    results = await asyncio.gather(*tasks, return_exceptions=True)

    tool_results = {
        "home": {},
        "away": {},
        "matchup": {},
        "error": None
    }

    # í™ˆíŒ€ ê²°ê³¼ ì²˜ë¦¬
    if isinstance(results[0], Exception):
        tool_results["error"] = str(results[0])
        tool_results["home"] = {"error": str(results[0])}
    else:
        tool_results["home"] = results[0]

    # ì›ì •íŒ€ ê²°ê³¼ ì²˜ë¦¬
    if away_team_id:
        if isinstance(results[1], Exception):
            tool_results["away"] = {"error": str(results[1])}
        else:
            tool_results["away"] = results[1]
            
        # ìƒëŒ€ ì „ì  ì²˜ë¦¬
        if "matchup" in focus:
            if len(tasks) > 2 and isinstance(results[2], Exception):
                tool_results["matchup"] = {"error": str(results[2])}
            elif len(tasks) > 2:
                tool_results["matchup"] = results[2]

    # ë ˆê±°ì‹œ êµ¬ì¡° í˜¸í™˜ì„± ìœ ì§€ (ë‹¨ì¼ íŒ€ ë¶„ì„ ìš”ì²­ ì‹œ)
    if not away_team_id:
        tool_results["team_summary"] = tool_results["home"].get("summary", {})
        tool_results["advanced_metrics"] = tool_results["home"].get("advanced", {})
        tool_results["recent_form"] = tool_results["home"].get("recent", {})

    return tool_results


def _safe_float(value: Any, default: float = 0.0) -> float:
    """None-safe float conversion for formatting."""
    if value is None:
        return default
    try:
        return float(value)
    except (ValueError, TypeError):
        return default


def _safe_int(value: Any, default: int = 0) -> int:
    """None-safe int conversion for formatting."""
    if value is None:
        return default
    try:
        return int(value)
    except (ValueError, TypeError):
        return default


def _remove_duplicate_json_start(text: str) -> str:
    """
    LLM ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ë°œìƒí•˜ëŠ” JSON ì‹œì‘ ë¶€ë¶„ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.

    ì¼ë¶€ LLMì€ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ "content restart" í˜„ìƒìœ¼ë¡œ ë™ì¼í•œ í•„ë“œë¥¼
    ë‘ ë²ˆ ì¶œë ¥í•˜ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì¤‘ë³µì„ ê°ì§€í•˜ê³  ì œê±°í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
        ì…ë ¥: '{"headline": "A",\\n"headline": "A", "sentiment": ...'
        ì¶œë ¥: '{"headline": "A", "sentiment": ...'

    Args:
        text: LLMì˜ ì›ì‹œ ì¶œë ¥ í…ìŠ¤íŠ¸

    Returns:
        ì¤‘ë³µì´ ì œê±°ëœ í…ìŠ¤íŠ¸
    """
    import re

    if not text or "{" not in text:
        return text

    # headline í•„ë“œ íŒ¨í„´ ì°¾ê¸°
    headline_pattern = r'"headline"\s*:\s*"[^"]*"'
    matches = list(re.finditer(headline_pattern, text))

    if len(matches) < 2:
        return text

    # ë‘ ê°œ ì´ìƒì˜ headline í•„ë“œê°€ ìˆìœ¼ë©´ ì¤‘ë³µ
    first_match = matches[0]
    second_match = matches[1]

    # ë‘ headline ê°’ì´ ë™ì¼í•œì§€ í™•ì¸
    first_value = text[first_match.start() : first_match.end()]
    second_value = text[second_match.start() : second_match.end()]

    if first_value == second_value:
        # ì¤‘ë³µ ë°œê²¬ - ë‘ ë²ˆì§¸ headline ì´í›„ ë‚´ìš©ë§Œ ìœ ì§€
        logger.warning(
            "[Coach] Duplicate JSON start detected, removing first occurrence"
        )

        # { + ë‘ ë²ˆì§¸ headlineë¶€í„°ì˜ ë‚´ìš©
        brace_pos = text.index("{")
        clean_text = text[brace_pos : brace_pos + 1]  # '{'

        # ë‘ ë²ˆì§¸ headline ì´í›„ ë‚´ìš©
        after_second = text[second_match.start() :]
        clean_text += after_second

        return clean_text

    return text


def _format_team_stats(team_data: Dict[str, Any], team_role: str = "Home") -> str:
    """ë‹¨ì¼ íŒ€ í†µê³„ í¬ë§·íŒ… í—¬í¼"""
    parts = []
    
    summary = team_data.get("summary", {})
    advanced = team_data.get("advanced", {})
    team_name = summary.get("team_name", "Unknown")
    
    parts.append(f"### [{team_role}] {team_name}")
    
    # í•µì‹¬ ì§€í‘œ
    if advanced.get("metrics"):
        batting = advanced["metrics"].get("batting", {})
        pitching = advanced["metrics"].get("pitching", {})
        rankings = advanced.get("rankings", {})
        
        parts.append("| ì§€í‘œ | ìˆ˜ì¹˜ | ìˆœìœ„ |")
        parts.append("|------|------|------|")
        if batting.get("ops"):
            parts.append(f"| OPS | {_safe_float(batting['ops']):.3f} | {rankings.get('batting_ops', '-')}|")
        if pitching.get("avg_era"):
            parts.append(f"| ERA | {_safe_float(pitching['avg_era']):.2f} | {pitching.get('era_rank', '-')}|")
        parts.append("")

    # ë¶ˆíœ
    fatigue = advanced.get("fatigue_index", {})
    if fatigue:
        parts.append(f"- **ë¶ˆíœ ë¹„ì¤‘**: {fatigue.get('bullpen_share', '-')}")
        parts.append(f"- **í”¼ë¡œë„ ìˆœìœ„**: {fatigue.get('bullpen_load_rank', '-')}")
        parts.append("")

    # ì£¼ìš” ì„ ìˆ˜ (ê°„ëµí™”)
    top_batters = summary.get("top_batters", [])[:3]
    if top_batters:
        parts.append("**ì£¼ìš” íƒ€ì**:")
        for b in top_batters:
             parts.append(f"- {b['player_name']}: OPS {_safe_float(b.get('ops')):.3f}, {b.get('home_runs')}HR")
    
    top_pitchers = summary.get("top_pitchers", [])[:3]
    if top_pitchers:
        parts.append("**ì£¼ìš” íˆ¬ìˆ˜**:")
        for p in top_pitchers:
            parts.append(f"- {p['player_name']}: ERA {_safe_float(p.get('era')):.2f}, {p.get('wins')}ìŠ¹")
    
    # ìµœê·¼ í¼ â€” DB schema: summary={wins,losses,draws,run_diff}, games=[{result:"Win"/"Loss"/"Draw", score:"5:3", run_diff, date, opponent}]
    recent = team_data.get("recent", {})
    if recent and recent.get("found"):
        parts.append("**ìµœê·¼ ê²½ê¸° íë¦„**:")
        r_summary = recent.get("summary", {})
        r_games = recent.get("games", [])
        wins = r_summary.get("wins", 0)
        losses = r_summary.get("losses", 0)
        draws = r_summary.get("draws", 0)
        parts.append(f"- ìµœê·¼ {len(r_games)}ê²½ê¸°: {wins}ìŠ¹ {losses}íŒ¨{f' {draws}ë¬´' if draws else ''}")
        run_diff = r_summary.get("run_diff")
        if run_diff is not None:
            parts.append(f"- ë“ì‹¤ ë§ˆì§„: {'+' if run_diff >= 0 else ''}{run_diff}")
        win_rate = r_summary.get("win_rate")
        if win_rate is not None:
            parts.append(f"- ìŠ¹ë¥ : {win_rate:.3f}")
        parts.append("")

    parts.append("")
    return "\n".join(parts)


def _format_coach_context(
    tool_results: Dict[str, Any],
    focus: List[str],
    game_context: Optional[str] = None,
    league_context: Optional[Dict[str, Any]] = None
) -> str:
    """
    Coach ì „ìš© ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬ë§·í•©ë‹ˆë‹¤.
    ë“€ì–¼ íŒ€ ë°ì´í„° ì§€ì›.
    """
    parts = []

    # 1. ë¦¬ê·¸/ê²½ê¸° ì»¨í…ìŠ¤íŠ¸
    if league_context:
        season = league_context.get("season")
        league_type = league_context.get("league_type")
        parts.append(f"## ğŸŸï¸ {season} ì‹œì¦Œ ì»¨í…ìŠ¤íŠ¸")
        
        if league_type == "POST":
            parts.append(f"**{league_context.get('round')} {league_context.get('game_no')}ì°¨ì „**")
        else:
            home = league_context.get("home", {})
            away = league_context.get("away", {})
            parts.append(f"- **Home**: {home.get('rank')}ìœ„ ({home.get('gamesBehind')} GB)")
            parts.append(f"- **Away**: {away.get('rank')}ìœ„ ({away.get('gamesBehind')} GB)")
        parts.append("")

    # 2. ê²½ê¸° ë³„ ëª¨ë“œ ì•ˆë‚´
    if game_context:
        parts.append("## âš ï¸ íŠ¹ì • ê²½ê¸° ë¶„ì„ ëª¨ë“œ")
        parts.append(f"**ë¶„ì„ ëŒ€ìƒ**: {game_context}")
        parts.append("")

    # 3. íŒ€ë³„ ë°ì´í„°
    if tool_results.get("home"):
        parts.append(_format_team_stats(tool_results["home"], "Home"))
        
    if tool_results.get("away"):
        parts.append(_format_team_stats(tool_results["away"], "Away"))

    # 4. ìƒëŒ€ ì „ì 
    matchup = tool_results.get("matchup", {})
    if matchup and matchup.get("games"):
        parts.append("### âš”ï¸ ë§ëŒ€ê²° ì „ì ")
        summary = matchup.get("summary", {})
        t1 = matchup.get("team1", "íŒ€1")
        t2 = matchup.get("team2", "íŒ€2")
        parts.append(
            f"- {t1} {summary.get('team1_wins', 0)}ìŠ¹ / "
            f"{t2} {summary.get('team2_wins', 0)}ìŠ¹ / "
            f"{summary.get('draws', 0)}ë¬´"
        )
        parts.append("| ë‚ ì§œ | ìŠ¤ì½”ì–´ | ê²°ê³¼ |")
        parts.append("|------|--------|------|")
        for g in matchup.get("games", [])[:3]:
            game_date = g.get("game_date", "")
            if hasattr(game_date, "strftime"):
                game_date = game_date.strftime("%Y-%m-%d")
            score = f"{g.get('home_score', 0)}:{g.get('away_score', 0)}"
            result_val = g.get("game_result", "")
            parts.append(f"| {game_date} | {score} | {result_val} |")
        parts.append("")

    return "\n".join(parts)


class AnalyzeRequest(BaseModel):
    team_id: Optional[str] = None  # deprecated â€” use home_team_id
    home_team_id: Optional[str] = None
    away_team_id: Optional[str] = None
    league_context: Optional[Dict[str, Any]] = None
    focus: List[str] = []
    game_id: Optional[str] = None
    question_override: Optional[str] = None

    @model_validator(mode="before")
    @classmethod
    def backfill_home_team_id(cls, values: Any) -> Any:
        """team_idë§Œ ë³´ë‚´ëŠ” ê¸°ì¡´ í˜¸ì¶œì„ home_team_idë¡œ ë§¤í•‘"""
        if isinstance(values, dict):
            if not values.get("home_team_id") and values.get("team_id"):
                values["home_team_id"] = values["team_id"]
        return values


@router.post("/analyze")
async def analyze_team(
    payload: AnalyzeRequest,
    agent: BaseballStatisticsAgent = Depends(get_agent),
    _: None = Depends(rate_limit_dependency),
):
    """
    íŠ¹ì • íŒ€(ë“¤)ì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ì„ ìš”ì²­í•©ë‹ˆë‹¤. 'The Coach' í˜ë¥´ì†Œë‚˜ê°€ ì ìš©ë©ë‹ˆë‹¤.
    """
    from sse_starlette.sse import EventSourceResponse
    import psycopg
    import hashlib

    # í•˜ìœ„ í˜¸í™˜ì„±ì€ model_validatorì—ì„œ ì²˜ë¦¬ë¨
    if not payload.home_team_id:
        raise HTTPException(status_code=400, detail="home_team_id ë˜ëŠ” team_idê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    try:
        request_id = uuid.uuid4().hex[:8]
        
        home_name = agent._convert_team_id_to_name(payload.home_team_id)
        away_name = agent._convert_team_id_to_name(payload.away_team_id) if payload.away_team_id else None
        
        if payload.question_override:
            query = payload.question_override
        else:
            query = _build_coach_query(
                home_name, 
                payload.focus, 
                opponent_name=away_name,
                league_context=payload.league_context
            )

        # ì—°ë„ ê²°ì • ë¡œì§
        now = datetime.now()
        target_year = now.year
        pre_season_notice = None

        if payload.league_context and payload.league_context.get("season"):
            raw_season = payload.league_context["season"]
            # seasonIdëŠ” KBO ì‹œì¦Œ ID (ì˜ˆ: 20255)ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì• 4ìë¦¬ë§Œ ì¶”ì¶œ
            try:
                target_year = int(str(raw_season)[:4])
            except (ValueError, TypeError):
                target_year = now.year
        elif now.month <= 3:
            target_year = now.year - 1
            pre_season_notice = f"NOTICE: í˜„ì¬ {now.year}ë…„ ì‹œì¦Œ ê°œë§‰ ì „ì´ë¯€ë¡œ, {target_year}ë…„ ì‹œì¦Œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤."

        year = target_year

        # Cache Key ìƒì„±
        cache_components = [
            payload.home_team_id,
            payload.away_team_id or "",
            str(year),
            json.dumps(payload.league_context or {}, sort_keys=True),
            ",".join(sorted(payload.focus)),
            payload.question_override or "",
            payload.game_id or "",
            "v4_dual",  # ë²„ì „ ì—…ë°ì´íŠ¸
        ]
        cache_key = hashlib.sha256("|".join(cache_components).encode()).hexdigest()

        logger.info(
            "[Coach Router] Analyzing %s vs %s (year=%d): %s... (CacheKey: %s)",
            home_name,
            away_name or "Single",
            year,
            query[:100],
            cache_key,
        )

        async def event_generator():
            try:
                total_start = perf_counter()
                
                # Phase 1: ì‹œì‘
                yield {
                    "event": "status",
                    "data": json.dumps({"message": "ì–‘ íŒ€ ì „ë ¥ ë¶„ì„ ì¤‘..."}, ensure_ascii=False),
                }

                pool = get_connection_pool()

                # Phase 0: ìºì‹œ í™•ì¸
                CACHE_TTL_HOURS = 168
                cached_data = None
                
                with pool.connection() as conn:
                    row = conn.execute(
                        """
                        INSERT INTO coach_analysis_cache (cache_key, team_id, year, prompt_version, model_name, status)
                        VALUES (%s, %s, %s, %s, %s, 'PENDING')
                        ON CONFLICT (cache_key) DO UPDATE
                            SET cache_key = coach_analysis_cache.cache_key
                        RETURNING status, response_json, (xmax = 0) AS inserted,
                                  (updated_at > now() - interval '7 days') AS is_valid
                        """,
                        (cache_key, payload.home_team_id, year, "v4_dual", "solar-pro-3"),
                    ).fetchone()
                    conn.commit()

                    if row:
                        status, cached_json, was_inserted, is_valid = row
                        if status == "COMPLETED" and cached_json and is_valid:
                            cached_data = cached_json
                            logger.info("[Coach] Cache HIT for %s", cache_key)
                        elif status == "COMPLETED" and cached_json and not is_valid:
                            conn.execute(
                                "UPDATE coach_analysis_cache SET status = 'PENDING', updated_at = now() WHERE cache_key = %s",
                                (cache_key,),
                            )
                            conn.commit()
                            logger.info("[Coach] Cache EXPIRED, recomputing")

                if cached_data:
                    cached_data = _normalize_cached_response(cached_data)
                    yield {
                        "event": "status",
                        "data": json.dumps({"message": "ë¶„ì„ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤..."}, ensure_ascii=False),
                    }
                    json_str = json.dumps(cached_data, ensure_ascii=False, indent=2)
                    yield {
                        "event": "message",
                        "data": json.dumps({"delta": json_str}, ensure_ascii=False),
                    }
                    yield {
                        "event": "meta",
                        "data": json.dumps({
                            "validation_status": "success",
                            "structured_response": cached_data,
                            "fast_path": True,
                            "cached": True,
                        }, ensure_ascii=False),
                    }
                    yield {"event": "done", "data": "[DONE]"}
                    return

                # ë„êµ¬ ì‹¤í–‰
                yield {
                    "event": "tool_start",
                    "data": json.dumps({"tool": "parallel_fetch_team_data"}, ensure_ascii=False),
                }

                tool_results = await _execute_coach_tools_parallel(
                    pool, payload.home_team_id, year, payload.focus, payload.away_team_id
                )
                
                yield {
                    "event": "tool_result",
                    "data": json.dumps({
                        "tool": "parallel_fetch_team_data",
                        "success": True,
                        "message": "ë°ì´í„° ì¡°íšŒ ì™„ë£Œ"
                    }, ensure_ascii=False),
                }

                # Phase 2: ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
                game_context = payload.question_override if payload.question_override else None
                # Game info fetching can be added here if needed, consistent with tool_results usage

                context = _format_coach_context(
                    tool_results, 
                    payload.focus, 
                    game_context,
                    payload.league_context
                )

                if pre_season_notice:
                    context = f"## ì¤‘ìš” ì•Œë¦¼\n{pre_season_notice}\n\n" + context

                # ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ (ê°„ì†Œí™”)
                # í™ˆíŒ€ ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
                home_data = tool_results.get("home", {})
                has_home_data = bool(home_data.get("summary")) or bool(home_data.get("advanced"))
                
                if not has_home_data:
                    logger.warning("[Coach] Data validation failed - skipping LLM call")
                    with pool.connection() as conn:
                        conn.execute(
                            "UPDATE coach_analysis_cache SET status = 'FAILED', error_message = %s, updated_at = now() WHERE cache_key = %s",
                            ("Data insufficient", cache_key),
                        )
                        conn.commit()
                        
                    fallback_response = json.dumps({
                         "headline": f"{home_name} ë°ì´í„°ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                         "sentiment": "neutral",
                         "key_metrics": [],
                         "analysis": {"strengths": [], "weaknesses": [], "risks": []},
                         "detailed_markdown": "## ë°ì´í„° ë¶€ì¡±\n\në°ì´í„°ë¥¼ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                         "coach_note": "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                    }, ensure_ascii=False)
                    
                    yield {"event": "message", "data": json.dumps({"delta": fallback_response}, ensure_ascii=False)}
                    yield {"event": "done", "data": "[DONE]"}
                    return

                # Phase 3: LLM í˜¸ì¶œ
                yield {
                    "event": "status",
                    "data": json.dumps({"message": "AI ì½”ì¹˜ê°€ ë¶„ì„ ë¦¬í¬íŠ¸ ì‘ì„± ì¤‘..."}, ensure_ascii=False),
                }

                coach_prompt = COACH_PROMPT_V2.format(question=query, context=context)
                messages = [{"role": "user", "content": coach_prompt}]
                
                coach_llm = get_coach_llm_generator()
                response_chunks = []
                
                async for chunk in coach_llm(messages):
                    response_chunks.append(chunk)
                    yield {
                        "event": "message",
                        "data": json.dumps({"delta": chunk}, ensure_ascii=False),
                    }
                full_response = "".join(response_chunks)
                full_response = _remove_duplicate_json_start(full_response)

                # Phase 4: ê²€ì¦ ë° ì €ì¥
                parsed_response, parse_error = parse_coach_response(full_response)
                
                with pool.connection() as conn:
                    if parsed_response:
                        conn.execute(
                            "UPDATE coach_analysis_cache SET status = 'COMPLETED', response_json = %s, updated_at = now() WHERE cache_key = %s",
                            (json.dumps(parsed_response.model_dump(), ensure_ascii=False), cache_key),
                        )
                    else:
                        conn.execute(
                            "UPDATE coach_analysis_cache SET status = 'FAILED', error_message = %s, updated_at = now() WHERE cache_key = %s",
                            (parse_error or "Validation failed", cache_key),
                        )
                    conn.commit()
                
                meta_payload = {
                     "verified": True,
                     "fast_path": True,
                     "validation_status": "success" if parsed_response else "fallback"
                }
                if parsed_response:
                    meta_payload["structured_response"] = parsed_response.model_dump()
                
                yield {
                    "event": "meta",
                    "data": json.dumps(meta_payload, ensure_ascii=False),
                }

                yield {"event": "done", "data": "[DONE]"}

            except Exception as e:
                logger.error(f"[Coach Streaming Error] {e}")
                # Cache fail logic
                try:
                    pool = get_connection_pool()
                    with pool.connection() as conn:
                        conn.execute(
                            "UPDATE coach_analysis_cache SET status = 'FAILED', error_message = %s, updated_at = now() WHERE cache_key = %s",
                            (str(e), cache_key),
                        )
                        conn.commit()
                except:
                    pass
                    
                yield {
                    "event": "error",
                    "data": json.dumps({"error": str(e)}, ensure_ascii=False),
                }

        return EventSourceResponse(
            event_generator(),
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no",
            },
        )

    except Exception as e:
        logger.error(f"[Coach Router] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================
# Legacy endpoint (ê¸°ì¡´ ë°©ì‹ ìœ ì§€, í•„ìš” ì‹œ ì‚¬ìš©)
# ============================================================


@router.post("/analyze-legacy")
async def analyze_team_legacy(
    payload: AnalyzeRequest,
    agent: BaseballStatisticsAgent = Depends(get_agent),
    _: None = Depends(rate_limit_dependency),
):
    """
    ê¸°ì¡´ ë°©ì‹ì˜ Coach ë¶„ì„ (ì „ì²´ ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©).
    Fast Pathì— ë¬¸ì œê°€ ìˆì„ ê²½ìš° ëŒ€ì•ˆìœ¼ë¡œ ì‚¬ìš©.
    """
    from sse_starlette.sse import EventSourceResponse

    try:
        primary_team_id = payload.home_team_id or payload.team_id
        if not primary_team_id:
            raise HTTPException(status_code=400, detail="home_team_id ë˜ëŠ” team_idê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        team_name = agent._convert_team_id_to_name(primary_team_id)

        if payload.question_override:
            query = payload.question_override
        else:
            query = _build_coach_query(team_name, payload.focus)

        logger.info(f"[Coach Router Legacy] Analyzing for {team_name}")

        context_data = {"persona": "coach", "team_id": primary_team_id}

        async def event_generator():
            try:
                async for event in agent.process_query_stream(
                    query, context=context_data
                ):
                    if event["type"] == "status":
                        yield {
                            "event": "status",
                            "data": json.dumps(
                                {"message": event["message"]}, ensure_ascii=False
                            ),
                        }
                    elif event["type"] == "tool_start":
                        yield {
                            "event": "tool_start",
                            "data": json.dumps(
                                {"tool": event["tool"]}, ensure_ascii=False
                            ),
                        }
                    elif event["type"] == "tool_result":
                        yield {
                            "event": "tool_result",
                            "data": json.dumps(
                                {
                                    "tool": event["tool"],
                                    "success": event["success"],
                                    "message": event["message"],
                                },
                                ensure_ascii=False,
                            ),
                        }
                    elif event["type"] == "answer_chunk":
                        yield {
                            "event": "message",
                            "data": json.dumps(
                                {"delta": event["content"]}, ensure_ascii=False
                            ),
                        }
                    elif event["type"] == "metadata":
                        meta_payload = {
                            "tool_calls": [
                                tc.to_dict() for tc in event["data"]["tool_calls"]
                            ],
                            "verified": event["data"]["verified"],
                            "data_sources": event["data"]["data_sources"],
                        }
                        yield {
                            "event": "meta",
                            "data": json.dumps(meta_payload, ensure_ascii=False),
                        }

                yield {"event": "done", "data": "[DONE]"}
            except Exception as e:
                logger.error(f"[Coach Legacy Streaming Error] {e}")
                import traceback

                logger.error(traceback.format_exc())
                yield {
                    "event": "error",
                    "data": json.dumps({"error": str(e)}, ensure_ascii=False),
                }

        return EventSourceResponse(
            event_generator(),
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no",
            },
        )

    except Exception as e:
        logger.error(f"[Coach Router Legacy] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
