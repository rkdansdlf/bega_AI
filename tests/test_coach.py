"""
Coach ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“ˆ.

Fast Path, ìºì‹±, ì‘ë‹µ ê²€ì¦ê¸° ë“±ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import pytest
import json
import asyncio
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from datetime import datetime

# ============================================================
# Coach Validator Tests
# ============================================================


class TestCoachValidator:
    """Coach ì‘ë‹µ ê²€ì¦ê¸° í…ŒìŠ¤íŠ¸"""

    def test_extract_json_from_pure_json(self):
        """ìˆœìˆ˜ JSON ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        from app.core.coach_validator import extract_json_from_response

        raw = '{"headline": "í…ŒìŠ¤íŠ¸", "sentiment": "neutral"}'
        result = extract_json_from_response(raw)
        assert result is not None
        assert "headline" in result

    def test_extract_json_from_code_block(self):
        """ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        from app.core.coach_validator import extract_json_from_response

        raw = """ì—¬ê¸° ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

```json
{"headline": "ì„ ë°œ ë¶•ê´´ê°€ ë¶ˆíœ ê³¼ë¶€í•˜ë¡œ ì§ê²°", "sentiment": "negative"}
```

ìœ„ ë¶„ì„ì„ ì°¸ê³ í•˜ì„¸ìš”."""

        result = extract_json_from_response(raw)
        assert result is not None
        data = json.loads(result)
        assert data["headline"] == "ì„ ë°œ ë¶•ê´´ê°€ ë¶ˆíœ ê³¼ë¶€í•˜ë¡œ ì§ê²°"
        assert data["sentiment"] == "negative"

    def test_extract_json_from_mixed_text(self):
        """í˜¼í•© í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        from app.core.coach_validator import extract_json_from_response

        raw = """ë¶„ì„ ì‹œì‘
{"headline": "í…ŒìŠ¤íŠ¸ í—¤ë“œë¼ì¸", "sentiment": "positive", "key_metrics": []}
ë¶„ì„ ì¢…ë£Œ"""

        result = extract_json_from_response(raw)
        assert result is not None
        data = json.loads(result)
        assert data["headline"] == "í…ŒìŠ¤íŠ¸ í—¤ë“œë¼ì¸"

    def test_parse_valid_coach_response(self):
        """ìœ íš¨í•œ Coach ì‘ë‹µ íŒŒì‹± í…ŒìŠ¤íŠ¸"""
        from app.core.coach_validator import parse_coach_response

        raw = """{
            "headline": "KIA íƒ€ì´ê±°ì¦ˆ ì„ ë°œì§„ ì•ˆì •í™” í•„ìš”",
            "sentiment": "negative",
            "key_metrics": [
                {"label": "íŒ€ ERA", "value": "4.52", "status": "ì£¼ì˜", "trend": "up", "is_critical": true}
            ],
            "analysis": {
                "strengths": ["íƒ€ì„  ìƒì‚°ë ¥ ì–‘í˜¸"],
                "weaknesses": ["ë¶ˆíœ ê³¼ë¶€í•˜"],
                "risks": [{"area": "ë¶ˆíœ", "level": 0, "description": "ë¦¬ê·¸ í‰ê·  ëŒ€ë¹„ 7%p ë†’ì€ ë¶ˆíœ ë¹„ì¤‘"}]
            },
            "detailed_markdown": "## ìƒì„¸ ë¶„ì„\\ní…ŒìŠ¤íŠ¸ ë‚´ìš©",
            "coach_note": "ì„ ë°œ ë¡œí…Œì´ì…˜ ì¬ì •ë¹„ê°€ ì‹œê¸‰í•©ë‹ˆë‹¤."
        }"""

        response, error = parse_coach_response(raw)
        assert response is not None
        assert response.headline == "KIA íƒ€ì´ê±°ì¦ˆ ì„ ë°œì§„ ì•ˆì •í™” í•„ìš”"
        assert response.sentiment == "negative"
        assert len(response.key_metrics) == 1
        assert response.key_metrics[0].is_critical is True
        assert len(response.analysis.risks) == 1
        assert response.analysis.risks[0].level == 0

    def test_parse_stable_trend_is_normalized(self):
        """key_metrics.trend='stable'ì„ neutralë¡œ ì •ê·œí™”"""
        from app.core.coach_validator import parse_coach_response

        raw = """{
            "headline": "KT ìœ„ì¦ˆ 2025ì‹œì¦Œ ì ê²€",
            "sentiment": "neutral",
            "key_metrics": [
                {"label": "ë¶ˆíœ ERA", "value": "4.21", "status": "warning", "trend": "stable", "is_critical": false}
            ],
            "analysis": {"strengths": [], "weaknesses": [], "risks": []},
            "detailed_markdown": "",
            "coach_note": "ë¶ˆíœ ìš´ìš©ì€ ë³´í•©ì„¸ì…ë‹ˆë‹¤."
        }"""

        response, error = parse_coach_response(raw)
        assert error is None
        assert response is not None
        assert response.key_metrics[0].trend == "neutral"

    def test_parse_invalid_json(self):
        """ì˜ëª»ëœ JSON íŒŒì‹± ì‹œ None ë°˜í™˜ í…ŒìŠ¤íŠ¸"""
        from app.core.coach_validator import parse_coach_response

        raw = "ì´ê²ƒì€ JSONì´ ì•„ë‹™ë‹ˆë‹¤."
        raw = "ì´ê²ƒì€ JSONì´ ì•„ë‹™ë‹ˆë‹¤."
        response, error = parse_coach_response(raw)
        assert response is None
        assert error is not None

    def test_parse_missing_headline_is_auto_recovered(self):
        """headline ëˆ„ë½ ì‹œ ìë™ ë³µêµ¬ í›„ íŒŒì‹± ì„±ê³µ"""
        from app.core.coach_validator import parse_coach_response_with_meta

        raw = """{
            "sentiment": "neutral",
            "key_metrics": [
                {"label": "OPS", "value": "0.812", "status": "good", "is_critical": true}
            ],
            "analysis": {
                "strengths": ["íƒ€ì„ ì˜ ì¶œë£¨ìœ¨ì´ ê¾¸ì¤€íˆ ìœ ì§€ë¨"],
                "weaknesses": [],
                "risks": []
            },
            "coach_note": "ì¤‘ì‹¬ íƒ€ì„  ìœ ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        }"""

        response, error, meta = parse_coach_response_with_meta(raw)
        assert error is None
        assert response is not None
        assert response.headline != ""
        assert meta["normalization_applied"] is True
        assert "derive_headline" in meta["normalization_reasons"]

    def test_parse_key_metric_value_is_truncated_before_validation(self):
        """key_metrics.valueê°€ 50ì ì´ˆê³¼ ì‹œ ì‚¬ì „ ì ˆë‹¨"""
        from app.core.coach_validator import parse_coach_response_with_meta

        long_value = "x" * 80
        raw = f"""{{
            "headline": "í…ŒìŠ¤íŠ¸ í—¤ë“œë¼ì¸",
            "sentiment": "neutral",
            "key_metrics": [
                {{"label": "ì¥ë¬¸ì§€í‘œ", "value": "{long_value}", "status": "warning", "is_critical": false}}
            ],
            "analysis": {{"strengths": [], "weaknesses": [], "risks": []}},
            "coach_note": "í…ŒìŠ¤íŠ¸ ë…¸íŠ¸ì…ë‹ˆë‹¤."
        }}"""

        response, error, meta = parse_coach_response_with_meta(raw)
        assert error is None
        assert response is not None
        assert len(response.key_metrics[0].value) <= 50
        assert response.key_metrics[0].value.endswith("...")
        assert meta["normalization_applied"] is True
        assert any(
            reason.startswith("truncate_key_metrics_value_")
            for reason in meta["normalization_reasons"]
        )

    def test_parse_is_critical_is_capped_to_two(self):
        """is_critical=trueê°€ 3ê°œ ì´ìƒì´ë©´ 2ê°œë¡œ ë³´ì •"""
        from app.core.coach_validator import parse_coach_response_with_meta

        raw = """{
            "headline": "í…ŒìŠ¤íŠ¸ í—¤ë“œë¼ì¸",
            "sentiment": "neutral",
            "key_metrics": [
                {"label": "ì§€í‘œ1", "value": "1", "status": "danger", "is_critical": true},
                {"label": "ì§€í‘œ2", "value": "2", "status": "danger", "is_critical": true},
                {"label": "ì§€í‘œ3", "value": "3", "status": "danger", "is_critical": true}
            ],
            "analysis": {"strengths": [], "weaknesses": [], "risks": []},
            "coach_note": "í…ŒìŠ¤íŠ¸ ë…¸íŠ¸ì…ë‹ˆë‹¤."
        }"""

        response, error, meta = parse_coach_response_with_meta(raw)
        assert error is None
        assert response is not None
        critical_count = sum(1 for metric in response.key_metrics if metric.is_critical)
        assert critical_count == 2
        assert "reduce_is_critical_3_to_2" in meta["normalization_reasons"]

    def test_parse_non_recoverable_shape_still_fails(self):
        """ì •ê·œí™”ë¡œ ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ í˜•íƒœëŠ” ê¸°ì¡´ì²˜ëŸ¼ ì‹¤íŒ¨"""
        from app.core.coach_validator import parse_coach_response

        raw = """{
            "headline": "í…ŒìŠ¤íŠ¸",
            "sentiment": "neutral",
            "key_metrics": [],
            "analysis": "invalid_type",
            "coach_note": "í…ŒìŠ¤íŠ¸ ë…¸íŠ¸ì…ë‹ˆë‹¤."
        }"""

        response, error = parse_coach_response(raw)
        assert response is None
        assert error is not None

    def test_validate_coach_response_warnings(self):
        """Coach ì‘ë‹µ ê²€ì¦ ê²½ê³  í…ŒìŠ¤íŠ¸"""
        from app.core.coach_validator import (
            CoachResponse,
            validate_coach_response,
            KeyMetric,
            AnalysisSection,
        )

        # í•µì‹¬ ì§€í‘œê°€ 4ê°œ ì´ìƒì¸ ê²½ìš° ê²½ê³ 
        response = CoachResponse(
            headline="í…ŒìŠ¤íŠ¸ í—¤ë“œë¼ì¸",
            sentiment="neutral",
            key_metrics=[
                KeyMetric(label=f"ì§€í‘œ{i}", value="1", status="ì–‘í˜¸", is_critical=True)
                for i in range(4)
            ],
            analysis=AnalysisSection(),
            coach_note="ì§§ìŒ",  # 20ì ë¯¸ë§Œ
        )

        warnings = validate_coach_response(response)
        assert len(warnings) >= 2  # í•µì‹¬ ì§€í‘œ ì´ˆê³¼ + coach_note ì§§ìŒ

    def test_format_coach_response_as_markdown(self):
        """Coach ì‘ë‹µ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ í…ŒìŠ¤íŠ¸"""
        from app.core.coach_validator import (
            CoachResponse,
            format_coach_response_as_markdown,
            KeyMetric,
            AnalysisSection,
            RiskItem,
        )

        response = CoachResponse(
            headline="íŒ€ ìƒíƒœ ì–‘í˜¸",
            sentiment="positive",
            key_metrics=[
                KeyMetric(
                    label="OPS",
                    value=".850",
                    status="ì–‘í˜¸",
                    trend="up",
                    is_critical=True,
                )
            ],
            analysis=AnalysisSection(
                strengths=["ê°•ë ¥í•œ íƒ€ì„ "],
                weaknesses=["ë¶ˆíœ ë¶ˆì•ˆ"],
                risks=[RiskItem(area="ë¶ˆíœ", level=1, description="ì£¼ì˜ í•„ìš”")],
            ),
            detailed_markdown="## ìƒì„¸ ë¶„ì„\ní…ŒìŠ¤íŠ¸ ë‚´ìš©",
            coach_note="ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
        )

        markdown = format_coach_response_as_markdown(response)
        assert "ğŸŸ¢" in markdown  # positive sentiment
        assert "OPS" in markdown
        assert "ê°•ë ¥í•œ íƒ€ì„ " in markdown
        assert "Coach's Note" in markdown


# ============================================================
# TTL Cache Tests
# ============================================================


class TestTTLCache:
    """TTL ìºì‹œ í…ŒìŠ¤íŠ¸"""

    def test_cache_set_and_get(self):
        """ìºì‹œ ì„¤ì • ë° ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        from app.tools.database_query import TTLCache

        cache = TTLCache(ttl_seconds=60)
        cache.set("test_key", {"data": "value"})

        result = cache.get("test_key")
        assert result is not None
        assert result["data"] == "value"

    def test_cache_miss(self):
        """ìºì‹œ ë¯¸ìŠ¤ í…ŒìŠ¤íŠ¸"""
        from app.tools.database_query import TTLCache

        cache = TTLCache(ttl_seconds=60)
        result = cache.get("nonexistent_key")
        assert result is None

    def test_cache_expiry(self):
        """ìºì‹œ ë§Œë£Œ í…ŒìŠ¤íŠ¸"""
        import time
        from app.tools.database_query import TTLCache

        cache = TTLCache(ttl_seconds=1)  # 1ì´ˆ TTL
        cache.set("expire_key", "value")

        # ì¦‰ì‹œ ì¡°íšŒ - ì¡´ì¬í•´ì•¼ í•¨
        assert cache.get("expire_key") == "value"

        # 1.5ì´ˆ ëŒ€ê¸° í›„ ì¡°íšŒ - ë§Œë£Œë¨
        time.sleep(1.5)
        assert cache.get("expire_key") is None

    def test_cache_max_size(self):
        """ìºì‹œ ìµœëŒ€ í¬ê¸° í…ŒìŠ¤íŠ¸"""
        from app.tools.database_query import TTLCache

        cache = TTLCache(ttl_seconds=60, max_size=3)

        # 4ê°œ í•­ëª© ì¶”ê°€ (ìµœëŒ€ í¬ê¸° ì´ˆê³¼)
        for i in range(4):
            cache.set(f"key_{i}", f"value_{i}")

        # ìµœëŒ€ 3ê°œë§Œ ìœ ì§€
        stats = cache.stats()
        assert stats["total_entries"] <= 3

    def test_cache_stats(self):
        """ìºì‹œ í†µê³„ í…ŒìŠ¤íŠ¸"""
        from app.tools.database_query import TTLCache

        cache = TTLCache(ttl_seconds=60, max_size=10)
        cache.set("key1", "value1")
        cache.set("key2", "value2")

        stats = cache.stats()
        assert stats["total_entries"] == 2
        assert stats["ttl_seconds"] == 60
        assert stats["max_size"] == 10


# ============================================================
# Tool Caller Parallel Execution Tests
# ============================================================


class TestToolCallerParallel:
    """ë³‘ë ¬ ë„êµ¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""

    def test_execute_multiple_tools_parallel(self):
        """
        ë³‘ë ¬ ë„êµ¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸

        [P3 Fix] ì‹œê°„ ê¸°ë°˜ ê²€ì¦ ëŒ€ì‹  Mock ê¸°ë°˜ ë™ì‹œì„± ê²€ì¦ìœ¼ë¡œ ìˆ˜ì •.
        ë‘ ë„êµ¬ê°€ ê±°ì˜ ë™ì‹œì— ì‹œì‘ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì—¬ CI í™˜ê²½ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ í†µê³¼.
        """

        async def _async_test():
            import time
            import threading
            from app.agents.tool_caller import ToolCaller, ToolCall, ToolResult

            caller = ToolCaller()
            call_times = []
            call_lock = threading.Lock()

            # ì‹œì‘ ì‹œê°„ì„ ê¸°ë¡í•˜ëŠ” ì¶”ì  ë„êµ¬
            def tracking_tool_1(param1: str) -> dict:
                with call_lock:
                    call_times.append(("tool_1", time.time()))
                time.sleep(0.1)  # ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
                return {"result": f"tool1: {param1}"}

            def tracking_tool_2(param2: str) -> dict:
                with call_lock:
                    call_times.append(("tool_2", time.time()))
                time.sleep(0.1)  # ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
                return {"result": f"tool2: {param2}"}

            caller.register_tool(
                "tracking_tool_1",
                "Tracking tool 1",
                {"param1": "Parameter 1"},
                tracking_tool_1,
            )
            caller.register_tool(
                "tracking_tool_2",
                "Tracking tool 2",
                {"param2": "Parameter 2"},
                tracking_tool_2,
            )

            # ë³‘ë ¬ ì‹¤í–‰
            tool_calls = [
                ToolCall("tracking_tool_1", {"param1": "test1"}),
                ToolCall("tracking_tool_2", {"param2": "test2"}),
            ]

            results = await caller.execute_multiple_tools_parallel(tool_calls)

            # ê²€ì¦: ë‘ ë„êµ¬ ëª¨ë‘ í˜¸ì¶œë¨
            assert len(call_times) == 2
            assert len(results) == 2
            assert all(r.success for r in results)

            # [P3 Fix] ë™ì‹œì„± ê²€ì¦: ë‘ ë„êµ¬ê°€ ê±°ì˜ ë™ì‹œì— ì‹œì‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
            # ìˆœì°¨ ì‹¤í–‰ì´ë©´ 0.1ì´ˆ ì´ìƒ ì°¨ì´, ë³‘ë ¬ì´ë©´ 0.05ì´ˆ ì´ë‚´
            time_diff = abs(call_times[0][1] - call_times[1][1])
            assert (
                time_diff < 0.05
            ), f"Tools not started concurrently: {time_diff:.3f}s apart"

        asyncio.run(_async_test())

    def test_parallel_execution_with_failure(self):
        """ë³‘ë ¬ ì‹¤í–‰ ì¤‘ ì¼ë¶€ ì‹¤íŒ¨ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""

        async def _async_test():
            from app.agents.tool_caller import ToolCaller, ToolCall

            caller = ToolCaller()

            def success_tool() -> dict:
                return {"status": "ok"}

            def fail_tool() -> dict:
                raise ValueError("Intentional failure")

            caller.register_tool("success_tool", "Success", {}, success_tool)
            caller.register_tool("fail_tool", "Fail", {}, fail_tool)

            tool_calls = [
                ToolCall("success_tool", {}),
                ToolCall("fail_tool", {}),
            ]

            results = await caller.execute_multiple_tools_parallel(tool_calls)

            assert results[0].success is True
            assert results[1].success is False

        asyncio.run(_async_test())


# ============================================================
# Coach Fast Path Integration Tests
# ============================================================


class TestCoachFastPath:
    """Coach Fast Path í†µí•© í…ŒìŠ¤íŠ¸"""

    def test_build_coach_query(self):
        """Coach ì¿¼ë¦¬ ë¹Œë“œ í…ŒìŠ¤íŠ¸"""
        from app.routers.coach import _build_coach_query

        # ê¸°ë³¸ ì¿¼ë¦¬
        query = _build_coach_query("KIA íƒ€ì´ê±°ì¦ˆ", [])
        assert "KIA íƒ€ì´ê±°ì¦ˆ" in query
        assert "ì¢…í•©ì ì¸ ì „ë ¥" in query

        # íŠ¹ì • focus
        query = _build_coach_query("ë‘ì‚° ë² ì–´ìŠ¤", ["bullpen", "batting"])
        assert "bullpen" in query or "ë¶ˆíœ" in query
        assert "batting" in query or "íƒ€ê²©" in query

    def test_format_coach_context(self):
        """Coach ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… í…ŒìŠ¤íŠ¸"""
        from app.routers.coach import _format_coach_context

        tool_results = {
            "home": {
                "summary": {
                    "team_name": "KIA íƒ€ì´ê±°ì¦ˆ",
                    "year": 2024,
                    "top_batters": [
                        {
                            "player_name": "ê¹€ë„ì˜",
                            "avg": 0.312,
                            "obp": 0.380,
                            "slg": 0.520,
                            "ops": 0.900,
                            "home_runs": 25,
                            "rbi": 80,
                        }
                    ],
                    "top_pitchers": [
                        {
                            "player_name": "ì–‘í˜„ì¢…",
                            "era": 3.45,
                            "whip": 1.12,
                            "wins": 12,
                            "losses": 5,
                            "saves": 0,
                            "innings_pitched": 150.0,
                        }
                    ],
                    "found": True,
                },
                "advanced": {
                    "team_name": "KIA íƒ€ì´ê±°ì¦ˆ",
                    "year": 2024,
                    "metrics": {
                        "batting": {"ops": 0.750, "avg": 0.280},
                        "pitching": {
                            "avg_era": 4.20,
                            "qs_rate": "45%",
                            "era_rank": "5ìœ„",
                        },
                    },
                    "fatigue_index": {
                        "bullpen_share": "35%",
                        "bullpen_load_rank": "3ìœ„ (ë†’ì„ìˆ˜ë¡ ê³¼ë¶€í•˜)",
                    },
                    "league_averages": {"bullpen_share": "30%", "era": 4.00},
                    "rankings": {"batting_ops": "4ìœ„", "batting_avg": "3ìœ„"},
                },
            }
        }

        context = _format_coach_context(tool_results, ["batting", "bullpen"])

        assert "KIA íƒ€ì´ê±°ì¦ˆ" in context
        assert "ê¹€ë„ì˜" in context
        assert "ì–‘í˜„ì¢…" in context
        assert "ë¶ˆíœ ë¹„ì¤‘" in context
        assert "35%" in context  # íŒ€ ë¶ˆíœ ë¹„ì¤‘


# ============================================================
# Run tests
# ============================================================

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
